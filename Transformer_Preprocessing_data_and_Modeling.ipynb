{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaruqAlfa/Stock-Mid-Price-Prediction-Based-on-LOB-Data-Using-the-Transformer-Model/blob/main/Transformer_Preprocessing_data_and_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv2x5p-Bi80h",
        "outputId": "9cab6d13-1619-4f52-8fc1-8b2279c055de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kode Pre Processing**"
      ],
      "metadata": {
        "id": "EUpxz_bNaaIN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D5AbXJIqcuk"
      },
      "source": [
        "## **Preprocessing menggunakan minmax scaller**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3nCDcf4odVx",
        "outputId": "e7478214-c0bf-4344-a1c6-9bbc43416f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== MULAI PREPROCESSING DATASET LOB ====\n",
            "Tahap 1: Membaca dan membersihkan data...\n",
            "Data awal: 1315 baris, 45 kolom\n",
            "5 baris pertama:\n",
            "             timestamp  last_price  percentage_change  high_price  low_price  \\\n",
            "0  2025-05-19 13:00:00        3170              -0.94        3240       3160   \n",
            "1  2025-05-19 13:01:00        3180              -0.62        3240       3160   \n",
            "2  2025-05-19 13:02:00        3180              -0.62        3240       3160   \n",
            "3  2025-05-19 13:03:00        3180              -0.62        3240       3160   \n",
            "4  2025-05-19 13:04:00        3180              -0.62        3240       3160   \n",
            "\n",
            "   bid_price_1  bid_volume_1  offer_price_1  offer_volume_1  bid_price_2  ...  \\\n",
            "0         3170        536200           3180          129500         3160  ...   \n",
            "1         3170        549100           3180          125200         3160  ...   \n",
            "2         3170        540900           3180           96500         3160  ...   \n",
            "3         3170        557700           3180           76100         3160  ...   \n",
            "4         3170        425500           3180           40700         3160  ...   \n",
            "\n",
            "   offer_price_8  offer_volume_8  bid_price_9  bid_volume_9  offer_price_9  \\\n",
            "0           3250         1437300         3090        437500           3260   \n",
            "1           3250         1427300         3090        437500           3260   \n",
            "2           3250         1426600         3090        437500           3260   \n",
            "3           3250         1427700         3090        436500           3260   \n",
            "4           3250         1424000         3090        436500           3260   \n",
            "\n",
            "   offer_volume_9  bid_price_10  bid_volume_10  offer_price_10  \\\n",
            "0          377400          3080         306600            3270   \n",
            "1          377400          3080         316600            3270   \n",
            "2          377400          3080         316800            3270   \n",
            "3          377400          3080         316600            3270   \n",
            "4          377400          3080         316600            3270   \n",
            "\n",
            "   offer_volume_10  \n",
            "0           336400  \n",
            "1           336400  \n",
            "2           341400  \n",
            "3           341400  \n",
            "4           341400  \n",
            "\n",
            "[5 rows x 45 columns]\n",
            "Jumlah baris yang seluruhnya NaN: 0\n",
            "Dataset awal memiliki 1315 baris dan 45 kolom.\n",
            "Setelah menghapus baris kosong: 1315 baris\n",
            "Tidak ada nilai yang hilang dalam dataset.\n",
            "Tidak ditemukan anomali dalam kolom harga.\n",
            "Dataset setelah dibersihkan memiliki 1315 baris.\n",
            "\n",
            "=== Data setelah dibersihkan ===\n",
            "               timestamp  last_price  percentage_change  high_price  \\\n",
            "0    2025-05-19 13:00:00        3170              -0.94        3240   \n",
            "1    2025-05-19 13:01:00        3180              -0.62        3240   \n",
            "2    2025-05-19 13:02:00        3180              -0.62        3240   \n",
            "3    2025-05-19 13:03:00        3180              -0.62        3240   \n",
            "4    2025-05-19 13:04:00        3180              -0.62        3240   \n",
            "...                  ...         ...                ...         ...   \n",
            "1310 2025-05-19 09:55:00         875               0.00         890   \n",
            "1311 2025-05-19 09:56:00         875               0.00         890   \n",
            "1312 2025-05-19 09:57:00         875               0.00         890   \n",
            "1313 2025-05-19 09:58:00         880               0.57         890   \n",
            "1314 2025-05-19 09:59:00         880               0.57         890   \n",
            "\n",
            "      low_price  bid_price_1  bid_volume_1  offer_price_1  offer_volume_1  \\\n",
            "0          3160         3170        536200           3180          129500   \n",
            "1          3160         3170        549100           3180          125200   \n",
            "2          3160         3170        540900           3180           96500   \n",
            "3          3160         3170        557700           3180           76100   \n",
            "4          3160         3170        425500           3180           40700   \n",
            "...         ...          ...           ...            ...             ...   \n",
            "1310        870          875        769300            880          813800   \n",
            "1311        870          875       1017000            880          841700   \n",
            "1312        870          875       1018400            880          841000   \n",
            "1313        870          875       1022500            880          839200   \n",
            "1314        870          875       1109200            880          804100   \n",
            "\n",
            "      bid_price_2  bid_volume_2  offer_price_2  offer_volume_2  bid_price_3  \\\n",
            "0            3160       1308200           3190          215500         3150   \n",
            "1            3160       1307800           3190          215500         3150   \n",
            "2            3160       1307700           3190          215500         3150   \n",
            "3            3160       1313400           3190          215600         3150   \n",
            "4            3160       1312600           3190          214400         3150   \n",
            "...           ...           ...            ...             ...          ...   \n",
            "1310          870       3465000            885         1710400          865   \n",
            "1311          870       3466200            885         1722400          865   \n",
            "1312          870       3603300            885         1721800          865   \n",
            "1313          870       3614800            885         1723100          865   \n",
            "1314          870       3707500            885         1729000          865   \n",
            "\n",
            "      bid_volume_3  offer_price_3  offer_volume_3  bid_price_4  bid_volume_4  \\\n",
            "0          1340700           3200          994900         3140        276000   \n",
            "1          1371100           3200          993900         3140        275900   \n",
            "2          1371100           3200          995200         3140        275900   \n",
            "3          1369800           3200          994700         3140        275900   \n",
            "4          1370100           3200         1000000         3140        275900   \n",
            "...            ...            ...             ...          ...           ...   \n",
            "1310       3205100            890         3687800          860       1878600   \n",
            "1311       3206400            890         3688200          860       1882700   \n",
            "1312       3207600            890         3688200          860       1882700   \n",
            "1313       3210600            890         3689200          860       1907700   \n",
            "1314       3216400            890         3688800          860       1907700   \n",
            "\n",
            "      offer_price_4  offer_volume_4  bid_price_5  bid_volume_5  offer_price_5  \\\n",
            "0              3210          512400         3130        184700           3220   \n",
            "1              3210          517400         3130        184600           3220   \n",
            "2              3210          518700         3130        184600           3220   \n",
            "3              3210          449200         3130        184600           3220   \n",
            "4              3210          449700         3130        184600           3220   \n",
            "...             ...             ...          ...           ...            ...   \n",
            "1310            895         3364600          855        609200            900   \n",
            "1311            895         3372800          855        615000            900   \n",
            "1312            895         3400600          855        616000            900   \n",
            "1313            895         3400600          855        618900            900   \n",
            "1314            895         3406100          855        612900            900   \n",
            "\n",
            "      offer_volume_5  bid_price_6  bid_volume_6  offer_price_6  \\\n",
            "0             841800         3120        281300           3230   \n",
            "1             841800         3120        281300           3230   \n",
            "2             847400         3120        281300           3230   \n",
            "3             847400         3120        281300           3230   \n",
            "4             847400         3120        281300           3230   \n",
            "...              ...          ...           ...            ...   \n",
            "1310         8515600          850       1143800            905   \n",
            "1311         8514400          850       1143900            905   \n",
            "1312         8514500          850       1143900            905   \n",
            "1313         8544500          850       1138900            905   \n",
            "1314         8569600          850       1140900            905   \n",
            "\n",
            "      offer_volume_6  bid_price_7  bid_volume_7  offer_price_7  \\\n",
            "0             739000         3110        765100           3240   \n",
            "1             739000         3110        765100           3240   \n",
            "2             739000         3110        765100           3240   \n",
            "3             739000         3110        766100           3240   \n",
            "4             739000         3110        766100           3240   \n",
            "...              ...          ...           ...            ...   \n",
            "1310         2768300          845        330300            910   \n",
            "1311         2768300          845        330400            910   \n",
            "1312         2768300          845        330400            910   \n",
            "1313         2768300          845        330400            910   \n",
            "1314         2768300          845        324400            910   \n",
            "\n",
            "      offer_volume_7  bid_price_8  bid_volume_8  offer_price_8  \\\n",
            "0             929000         3100       1302800           3250   \n",
            "1             929000         3100       1302800           3250   \n",
            "2             928600         3100       1302800           3250   \n",
            "3             928600         3100       1305300           3250   \n",
            "4             927600         3100       1305300           3250   \n",
            "...              ...          ...           ...            ...   \n",
            "1310         3035800          840       1061400            915   \n",
            "1311         3035800          840       1061400            915   \n",
            "1312         3035800          840       1061400            915   \n",
            "1313         3035800          840       1061400            915   \n",
            "1314         3035800          840       1061400            915   \n",
            "\n",
            "      offer_volume_8  bid_price_9  bid_volume_9  offer_price_9  \\\n",
            "0            1437300         3090        437500           3260   \n",
            "1            1427300         3090        437500           3260   \n",
            "2            1426600         3090        437500           3260   \n",
            "3            1427700         3090        436500           3260   \n",
            "4            1424000         3090        436500           3260   \n",
            "...              ...          ...           ...            ...   \n",
            "1310         1400900          835        609900            920   \n",
            "1311         1400900          835        615800            920   \n",
            "1312         1400900          835        615800            920   \n",
            "1313         1400900          835        615800            920   \n",
            "1314         1400900          835        615800            920   \n",
            "\n",
            "      offer_volume_9  bid_price_10  bid_volume_10  offer_price_10  \\\n",
            "0             377400          3080         306600            3270   \n",
            "1             377400          3080         316600            3270   \n",
            "2             377400          3080         316800            3270   \n",
            "3             377400          3080         316600            3270   \n",
            "4             377400          3080         316600            3270   \n",
            "...              ...           ...            ...             ...   \n",
            "1310         1885100           830         497400             925   \n",
            "1311         1885100           830         538000             925   \n",
            "1312         1885100           830         538000             925   \n",
            "1313         1885100           830         513000             925   \n",
            "1314         1785100           830         513000             925   \n",
            "\n",
            "      offer_volume_10  \n",
            "0              336400  \n",
            "1              336400  \n",
            "2              341400  \n",
            "3              341400  \n",
            "4              341400  \n",
            "...               ...  \n",
            "1310          2213100  \n",
            "1311          2213100  \n",
            "1312          2233600  \n",
            "1313          2233600  \n",
            "1314          2233600  \n",
            "\n",
            "[1315 rows x 45 columns]\n",
            "\n",
            "Tahap 2: Melakukan sinkronisasi waktu...\n",
            "Data sebelum resampling: 1315 baris\n",
            "Dataset setelah sinkronisasi waktu memiliki 1255 baris.\n",
            "\n",
            "=== Data setelah sinkronisasi waktu ===\n",
            "               timestamp  last_price  high_price  low_price  bid_price_1  \\\n",
            "0    2025-05-19 09:00:00       885.0       890.0      875.0        880.0   \n",
            "1    2025-05-19 09:01:00       880.0       890.0      875.0        880.0   \n",
            "2    2025-05-19 09:02:00       885.0       890.0      875.0        880.0   \n",
            "3    2025-05-19 09:03:00       880.0       890.0      875.0        880.0   \n",
            "4    2025-05-19 09:04:00       885.0       890.0      875.0        880.0   \n",
            "...                  ...         ...         ...        ...          ...   \n",
            "1250 2025-07-03 15:49:00      2590.0      2670.0     2590.0       2590.0   \n",
            "1251 2025-07-03 15:50:00      2600.0      2670.0     2590.0       2600.0   \n",
            "1252 2025-07-03 15:51:00      2600.0      2670.0     2590.0       2600.0   \n",
            "1253 2025-07-03 15:52:00      2600.0      2670.0     2590.0       2600.0   \n",
            "1254 2025-07-03 15:53:00      2600.0      2670.0     2590.0       2600.0   \n",
            "\n",
            "      offer_price_1  bid_price_2  offer_price_2  bid_price_3  offer_price_3  \\\n",
            "0             885.0        875.0          890.0        870.0          895.0   \n",
            "1             885.0        875.0          890.0        870.0          895.0   \n",
            "2             885.0        875.0          890.0        870.0          895.0   \n",
            "3             885.0        875.0          890.0        870.0          895.0   \n",
            "4             885.0        875.0          890.0        870.0          895.0   \n",
            "...             ...          ...            ...          ...            ...   \n",
            "1250         2600.0       2580.0         2610.0       2570.0         2620.0   \n",
            "1251         2610.0       2590.0         2620.0       2580.0         2630.0   \n",
            "1252         2610.0       2590.0         2620.0       2580.0         2630.0   \n",
            "1253         2610.0       2590.0         2620.0       2580.0         2630.0   \n",
            "1254         2610.0       2590.0         2620.0       2580.0         2630.0   \n",
            "\n",
            "      bid_price_4  offer_price_4  bid_price_5  offer_price_5  bid_price_6  \\\n",
            "0           865.0          900.0        860.0          905.0        855.0   \n",
            "1           865.0          900.0        860.0          905.0        855.0   \n",
            "2           865.0          900.0        860.0          905.0        855.0   \n",
            "3           865.0          900.0        860.0          905.0        855.0   \n",
            "4           865.0          900.0        860.0          905.0        855.0   \n",
            "...           ...            ...          ...            ...          ...   \n",
            "1250       2560.0         2630.0       2550.0         2640.0       2540.0   \n",
            "1251       2570.0         2640.0       2560.0         2650.0       2550.0   \n",
            "1252       2570.0         2640.0       2560.0         2650.0       2550.0   \n",
            "1253       2570.0         2640.0       2560.0         2650.0       2550.0   \n",
            "1254       2570.0         2640.0       2560.0         2650.0       2550.0   \n",
            "\n",
            "      offer_price_6  bid_price_7  offer_price_7  bid_price_8  offer_price_8  \\\n",
            "0             910.0        850.0          915.0        845.0          920.0   \n",
            "1             910.0        850.0          915.0        845.0          920.0   \n",
            "2             910.0        850.0          915.0        845.0          920.0   \n",
            "3             910.0        850.0          915.0        845.0          920.0   \n",
            "4             910.0        850.0          915.0        845.0          920.0   \n",
            "...             ...          ...            ...          ...            ...   \n",
            "1250         2650.0       2530.0         2660.0       2520.0         2670.0   \n",
            "1251         2660.0       2540.0         2670.0       2530.0         2680.0   \n",
            "1252         2660.0       2540.0         2670.0       2530.0         2680.0   \n",
            "1253         2660.0       2540.0         2670.0       2530.0         2680.0   \n",
            "1254         2660.0       2540.0         2670.0       2530.0         2680.0   \n",
            "\n",
            "      bid_price_9  offer_price_9  bid_price_10  offer_price_10  bid_volume_1  \\\n",
            "0           840.0          925.0         835.0           930.0       2581600   \n",
            "1           840.0          925.0         835.0           930.0       2584300   \n",
            "2           840.0          925.0         835.0           930.0       2682200   \n",
            "3           840.0          925.0         835.0           930.0       2683500   \n",
            "4           840.0          925.0         835.0           930.0       2762900   \n",
            "...           ...            ...           ...             ...           ...   \n",
            "1250       2510.0         2680.0        2500.0          2690.0         76400   \n",
            "1251       2520.0         2690.0        2510.0          2700.0         23700   \n",
            "1252       2520.0         2690.0        2510.0          2700.0         23700   \n",
            "1253       2520.0         2690.0        2510.0          2700.0         23700   \n",
            "1254       2520.0         2690.0        2510.0          2700.0         23700   \n",
            "\n",
            "      offer_volume_1  bid_volume_2  offer_volume_2  bid_volume_3  \\\n",
            "0            1073500       4702400         2783100       3611600   \n",
            "1            1086900       4738000         2811200       3622300   \n",
            "2            1043500       4729100         2847200       3612500   \n",
            "3            1043500       4770300         2993200       3612800   \n",
            "4            1061100       4610800         3011400       3612800   \n",
            "...              ...           ...             ...           ...   \n",
            "1250           40200        333400           60900        190900   \n",
            "1251           60800        144000          135100        327600   \n",
            "1252           60800        144000          135100        327600   \n",
            "1253           60800        144000          135100        327600   \n",
            "1254           60800        144000          135100        327600   \n",
            "\n",
            "      offer_volume_3  bid_volume_4  offer_volume_4  bid_volume_5  \\\n",
            "0            3830400       1448400         8121500       1738200   \n",
            "1            3900400       1453500         8126500       1733800   \n",
            "2            3900400       1566300         8128100       1743900   \n",
            "3            3858900       1566300         8141100       1743900   \n",
            "4            3850400       1566300         8141100       1745900   \n",
            "...              ...           ...             ...           ...   \n",
            "1250          115300        362700          144800        673700   \n",
            "1251          144800        194900          417000        356700   \n",
            "1252          144800        194900          417000        356700   \n",
            "1253          144800        194900          417000        356700   \n",
            "1254          144800        194900          417000        356700   \n",
            "\n",
            "      offer_volume_5  bid_volume_6  offer_volume_6  bid_volume_7  \\\n",
            "0            2778300        802900         2970900       1778100   \n",
            "1            2778300        804400         2977400       1783100   \n",
            "2            2778300        804400         2977400       1783000   \n",
            "3            2818300        804400         2975900       1783200   \n",
            "4            2818300        804400         2975900       1778200   \n",
            "...              ...           ...             ...           ...   \n",
            "1250          417000        218500          470100        189600   \n",
            "1251          470800        673700          360900        218500   \n",
            "1252          470800        673700          360900        218500   \n",
            "1253          470800        673700          360900        218500   \n",
            "1254          470800        673700          360900        218500   \n",
            "\n",
            "      offer_volume_7  bid_volume_8  offer_volume_8  bid_volume_9  \\\n",
            "0            1183100        444300         1856200       1056000   \n",
            "1            1189100        446300         1856200       1056500   \n",
            "2            1189100        446300         1857200       1056500   \n",
            "3            1189100        446300         1858700       1056500   \n",
            "4            1191600        446300         1859100       1056500   \n",
            "...              ...           ...             ...           ...   \n",
            "1250          361000        110200          766700        150900   \n",
            "1251          766600        191600          309400        110200   \n",
            "1252          766600        191600          309400        110200   \n",
            "1253          766600        191600          309400        110200   \n",
            "1254          766600        191600          309400        110200   \n",
            "\n",
            "      offer_volume_9  bid_volume_10  offer_volume_10  percentage_change  \n",
            "0            2212700         584300          1110800               1.14  \n",
            "1            2210200         584300          1098300               0.57  \n",
            "2            2195300         584300          1113200               1.14  \n",
            "3            2195300         584300          1113200               0.57  \n",
            "4            2195300         584300          1113200               1.14  \n",
            "...              ...            ...              ...                ...  \n",
            "1250          309600         619000           329200              -1.15  \n",
            "1251          329200         150900          1032900              -0.76  \n",
            "1252          329200         150900          1032900              -0.76  \n",
            "1253          329200         150900          1032900              -0.76  \n",
            "1254          329200         150900          1032900              -0.76  \n",
            "\n",
            "[1255 rows x 45 columns]\n",
            "\n",
            "Tahap 3 & 4: Menghitung mid-price...\n",
            "Ditemukan 10 kolom bid dan 10 kolom offer\n",
            "Mid-price telah ditambahkan untuk 10 level dan mid-price gabungan.\n",
            "\n",
            "=== Kolom mid-price ===\n",
            "               timestamp  mid_price_1  mid_price_2  mid_price_3  mid_price_4  \\\n",
            "0    2025-05-19 09:00:00        882.5        882.5        882.5        882.5   \n",
            "1    2025-05-19 09:01:00        882.5        882.5        882.5        882.5   \n",
            "2    2025-05-19 09:02:00        882.5        882.5        882.5        882.5   \n",
            "3    2025-05-19 09:03:00        882.5        882.5        882.5        882.5   \n",
            "4    2025-05-19 09:04:00        882.5        882.5        882.5        882.5   \n",
            "...                  ...          ...          ...          ...          ...   \n",
            "1250 2025-07-03 15:49:00       2595.0       2595.0       2595.0       2595.0   \n",
            "1251 2025-07-03 15:50:00       2605.0       2605.0       2605.0       2605.0   \n",
            "1252 2025-07-03 15:51:00       2605.0       2605.0       2605.0       2605.0   \n",
            "1253 2025-07-03 15:52:00       2605.0       2605.0       2605.0       2605.0   \n",
            "1254 2025-07-03 15:53:00       2605.0       2605.0       2605.0       2605.0   \n",
            "\n",
            "      mid_price_5  mid_price_6  mid_price_7  mid_price_8  mid_price_9  \\\n",
            "0           882.5        882.5        882.5        882.5        882.5   \n",
            "1           882.5        882.5        882.5        882.5        882.5   \n",
            "2           882.5        882.5        882.5        882.5        882.5   \n",
            "3           882.5        882.5        882.5        882.5        882.5   \n",
            "4           882.5        882.5        882.5        882.5        882.5   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "1250       2595.0       2595.0       2595.0       2595.0       2595.0   \n",
            "1251       2605.0       2605.0       2605.0       2605.0       2605.0   \n",
            "1252       2605.0       2605.0       2605.0       2605.0       2605.0   \n",
            "1253       2605.0       2605.0       2605.0       2605.0       2605.0   \n",
            "1254       2605.0       2605.0       2605.0       2605.0       2605.0   \n",
            "\n",
            "      mid_price_10  mid_price  \n",
            "0            882.5      882.5  \n",
            "1            882.5      882.5  \n",
            "2            882.5      882.5  \n",
            "3            882.5      882.5  \n",
            "4            882.5      882.5  \n",
            "...            ...        ...  \n",
            "1250        2595.0     2595.0  \n",
            "1251        2605.0     2605.0  \n",
            "1252        2605.0     2605.0  \n",
            "1253        2605.0     2605.0  \n",
            "1254        2605.0     2605.0  \n",
            "\n",
            "[1255 rows x 12 columns]\n",
            "\n",
            "Tahap 5: Melakukan normalisasi dan standarisasi...\n",
            "Data sebelum scaling: 1255 baris, 56 kolom\n",
            "Data telah distandarisasi dengan mean=0, std=1.\n",
            "Data setelah scaling: 1255 baris, 56 kolom\n",
            "\n",
            "=== Data setelah scaling ===\n",
            "               timestamp  last_price  high_price  low_price  bid_price_1  \\\n",
            "0    2025-05-19 09:00:00   -1.474088   -1.445519  -1.476078    -1.477306   \n",
            "1    2025-05-19 09:01:00   -1.476387   -1.445519  -1.476078    -1.477306   \n",
            "2    2025-05-19 09:02:00   -1.474088   -1.445519  -1.476078    -1.477306   \n",
            "3    2025-05-19 09:03:00   -1.476387   -1.445519  -1.476078    -1.477306   \n",
            "4    2025-05-19 09:04:00   -1.474088   -1.445519  -1.476078    -1.477306   \n",
            "...                  ...         ...         ...        ...          ...   \n",
            "1250 2025-07-03 15:49:00   -0.690344   -0.666250  -0.676259    -0.689894   \n",
            "1251 2025-07-03 15:50:00   -0.685748   -0.666250  -0.676259    -0.685289   \n",
            "1252 2025-07-03 15:51:00   -0.685748   -0.666250  -0.676259    -0.685289   \n",
            "1253 2025-07-03 15:52:00   -0.685748   -0.666250  -0.676259    -0.685289   \n",
            "1254 2025-07-03 15:53:00   -0.685748   -0.666250  -0.676259    -0.685289   \n",
            "\n",
            "      offer_price_1  bid_price_2  offer_price_2  bid_price_3  offer_price_3  \\\n",
            "0         -1.477050    -1.477582      -1.476773    -1.477859      -1.476497   \n",
            "1         -1.477050    -1.477582      -1.476773    -1.477859      -1.476497   \n",
            "2         -1.477050    -1.477582      -1.476773    -1.477859      -1.476497   \n",
            "3         -1.477050    -1.477582      -1.476773    -1.477859      -1.476497   \n",
            "4         -1.477050    -1.477582      -1.476773    -1.477859      -1.476497   \n",
            "...             ...          ...            ...          ...            ...   \n",
            "1250      -0.689790    -0.690027      -0.689656    -0.690161      -0.689522   \n",
            "1251      -0.685199    -0.685408      -0.685080    -0.685528      -0.684960   \n",
            "1252      -0.685199    -0.685408      -0.685080    -0.685528      -0.684960   \n",
            "1253      -0.685199    -0.685408      -0.685080    -0.685528      -0.684960   \n",
            "1254      -0.685199    -0.685408      -0.685080    -0.685528      -0.684960   \n",
            "\n",
            "      bid_price_4  offer_price_4  bid_price_5  offer_price_5  bid_price_6  \\\n",
            "0       -1.478135      -1.476220    -1.478411      -1.475943    -1.478687   \n",
            "1       -1.478135      -1.476220    -1.478411      -1.475943    -1.478687   \n",
            "2       -1.478135      -1.476220    -1.478411      -1.475943    -1.478687   \n",
            "3       -1.478135      -1.476220    -1.478411      -1.475943    -1.478687   \n",
            "4       -1.478135      -1.476220    -1.478411      -1.475943    -1.478687   \n",
            "...           ...            ...          ...            ...          ...   \n",
            "1250    -0.690295      -0.689388    -0.690428      -0.689255    -0.690562   \n",
            "1251    -0.685647      -0.684840    -0.685766      -0.684721    -0.685884   \n",
            "1252    -0.685647      -0.684840    -0.685766      -0.684721    -0.685884   \n",
            "1253    -0.685647      -0.684840    -0.685766      -0.684721    -0.685884   \n",
            "1254    -0.685647      -0.684840    -0.685766      -0.684721    -0.685884   \n",
            "\n",
            "      offer_price_6  bid_price_7  offer_price_7  bid_price_8  offer_price_8  \\\n",
            "0         -1.475666    -1.478963      -1.475389    -1.479239      -1.475113   \n",
            "1         -1.475666    -1.478963      -1.475389    -1.479239      -1.475113   \n",
            "2         -1.475666    -1.478963      -1.475389    -1.479239      -1.475113   \n",
            "3         -1.475666    -1.478963      -1.475389    -1.479239      -1.475113   \n",
            "4         -1.475666    -1.478963      -1.475389    -1.479239      -1.475113   \n",
            "...             ...          ...            ...          ...            ...   \n",
            "1250      -0.689121    -0.690695      -0.688988    -0.690829      -0.688854   \n",
            "1251      -0.684601    -0.686003      -0.684481    -0.686122      -0.684361   \n",
            "1252      -0.684601    -0.686003      -0.684481    -0.686122      -0.684361   \n",
            "1253      -0.684601    -0.686003      -0.684481    -0.686122      -0.684361   \n",
            "1254      -0.684601    -0.686003      -0.684481    -0.686122      -0.684361   \n",
            "\n",
            "      bid_price_9  offer_price_9  bid_price_10  offer_price_10  bid_volume_1  \\\n",
            "0       -1.479514      -1.474836     -1.479790       -1.474560      3.014902   \n",
            "1       -1.479514      -1.474836     -1.479790       -1.474560      3.018638   \n",
            "2       -1.479514      -1.474836     -1.479790       -1.474560      3.154086   \n",
            "3       -1.479514      -1.474836     -1.479790       -1.474560      3.155884   \n",
            "4       -1.479514      -1.474836     -1.479790       -1.474560      3.265737   \n",
            "...           ...            ...           ...             ...           ...   \n",
            "1250    -0.690962      -0.688721     -0.691095       -0.688587     -0.451120   \n",
            "1251    -0.686240      -0.684241     -0.686358       -0.684121     -0.524032   \n",
            "1252    -0.686240      -0.684241     -0.686358       -0.684121     -0.524032   \n",
            "1253    -0.686240      -0.684241     -0.686358       -0.684121     -0.524032   \n",
            "1254    -0.686240      -0.684241     -0.686358       -0.684121     -0.524032   \n",
            "\n",
            "      offer_volume_1  bid_volume_2  offer_volume_2  bid_volume_3  \\\n",
            "0           2.941921      4.648515        3.232278      4.004396   \n",
            "1           2.989827      4.689446        3.272667      4.019315   \n",
            "2           2.834671      4.679214        3.324411      4.005650   \n",
            "3           2.834671      4.726583        3.534261      4.006069   \n",
            "4           2.897591      4.543198        3.560420      4.006069   \n",
            "...              ...           ...             ...           ...   \n",
            "1250       -0.752150     -0.374744       -0.680418     -0.765096   \n",
            "1251       -0.678504     -0.592507       -0.573768     -0.574495   \n",
            "1252       -0.678504     -0.592507       -0.573768     -0.574495   \n",
            "1253       -0.678504     -0.592507       -0.573768     -0.574495   \n",
            "1254       -0.678504     -0.592507       -0.573768     -0.574495   \n",
            "\n",
            "      offer_volume_3  bid_volume_4  offer_volume_4  bid_volume_5  \\\n",
            "0           3.848743      1.919713        5.180431      2.770064   \n",
            "1           3.934381      1.930663        5.184004      2.760354   \n",
            "2           3.934381      2.172868        5.185147      2.782643   \n",
            "3           3.883610      2.172868        5.194437      2.782643   \n",
            "4           3.873211      2.172868        5.194437      2.787056   \n",
            "...              ...           ...             ...           ...   \n",
            "1250       -0.696296     -0.411508       -0.519925      0.420969   \n",
            "1251       -0.660206     -0.771809       -0.325404     -0.278574   \n",
            "1252       -0.660206     -0.771809       -0.325404     -0.278574   \n",
            "1253       -0.660206     -0.771809       -0.325404     -0.278574   \n",
            "1254       -0.660206     -0.771809       -0.325404     -0.278574   \n",
            "\n",
            "      offer_volume_5  bid_volume_6  offer_volume_6  bid_volume_7  \\\n",
            "0           1.636445      1.001249        3.102550      4.488365   \n",
            "1           1.636445      1.005180        3.112144      4.505101   \n",
            "2           1.636445      1.005180        3.112144      4.504766   \n",
            "3           1.670741      1.005180        3.109930      4.505436   \n",
            "4           1.670741      1.005180        3.109930      4.488700   \n",
            "...              ...           ...             ...           ...   \n",
            "1250       -0.388109     -0.530462       -0.588639     -0.828782   \n",
            "1251       -0.341981      0.662616       -0.749819     -0.732046   \n",
            "1252       -0.341981      0.662616       -0.749819     -0.732046   \n",
            "1253       -0.341981      0.662616       -0.749819     -0.732046   \n",
            "1254       -0.341981      0.662616       -0.749819     -0.732046   \n",
            "\n",
            "      offer_volume_7  bid_volume_8  offer_volume_8  bid_volume_9  \\\n",
            "0           0.222470      0.287246        1.859425      2.359117   \n",
            "1           0.229601      0.294647        1.859425      2.360877   \n",
            "2           0.229601      0.294647        1.861307      2.360877   \n",
            "3           0.229601      0.294647        1.864132      2.360877   \n",
            "4           0.232572      0.294647        1.864885      2.360877   \n",
            "...              ...           ...             ...           ...   \n",
            "1250       -0.754601     -0.949026       -0.191918     -0.827300   \n",
            "1251       -0.272543     -0.647821       -1.052936     -0.970585   \n",
            "1252       -0.272543     -0.647821       -1.052936     -0.970585   \n",
            "1253       -0.272543     -0.647821       -1.052936     -0.970585   \n",
            "1254       -0.272543     -0.647821       -1.052936     -0.970585   \n",
            "\n",
            "      offer_volume_9  bid_volume_10  offer_volume_10  percentage_change  \\\n",
            "0           2.980481       1.325000         0.751123           0.430880   \n",
            "1           2.975630       1.325000         0.728567           0.142248   \n",
            "2           2.946722       1.325000         0.755453           0.430880   \n",
            "3           2.946722       1.325000         0.755453           0.142248   \n",
            "4           2.946722       1.325000         0.755453           0.430880   \n",
            "...              ...            ...              ...                ...   \n",
            "1250       -0.711780       1.483285        -0.659220          -0.728712   \n",
            "1251       -0.673754      -0.651963         0.610558          -0.531227   \n",
            "1252       -0.673754      -0.651963         0.610558          -0.531227   \n",
            "1253       -0.673754      -0.651963         0.610558          -0.531227   \n",
            "1254       -0.673754      -0.651963         0.610558          -0.531227   \n",
            "\n",
            "      mid_price_1  mid_price_2  mid_price_3  mid_price_4  mid_price_5  \\\n",
            "0       -1.477178    -1.477178    -1.477178    -1.477178    -1.477178   \n",
            "1       -1.477178    -1.477178    -1.477178    -1.477178    -1.477178   \n",
            "2       -1.477178    -1.477178    -1.477178    -1.477178    -1.477178   \n",
            "3       -1.477178    -1.477178    -1.477178    -1.477178    -1.477178   \n",
            "4       -1.477178    -1.477178    -1.477178    -1.477178    -1.477178   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "1250    -0.689842    -0.689842    -0.689842    -0.689842    -0.689842   \n",
            "1251    -0.685244    -0.685244    -0.685244    -0.685244    -0.685244   \n",
            "1252    -0.685244    -0.685244    -0.685244    -0.685244    -0.685244   \n",
            "1253    -0.685244    -0.685244    -0.685244    -0.685244    -0.685244   \n",
            "1254    -0.685244    -0.685244    -0.685244    -0.685244    -0.685244   \n",
            "\n",
            "      mid_price_6  mid_price_7  mid_price_8  mid_price_9  mid_price_10  \\\n",
            "0       -1.477178    -1.477178    -1.477178    -1.477178     -1.477178   \n",
            "1       -1.477178    -1.477178    -1.477178    -1.477178     -1.477178   \n",
            "2       -1.477178    -1.477178    -1.477178    -1.477178     -1.477178   \n",
            "3       -1.477178    -1.477178    -1.477178    -1.477178     -1.477178   \n",
            "4       -1.477178    -1.477178    -1.477178    -1.477178     -1.477178   \n",
            "...           ...          ...          ...          ...           ...   \n",
            "1250    -0.689842    -0.689842    -0.689842    -0.689842     -0.689842   \n",
            "1251    -0.685244    -0.685244    -0.685244    -0.685244     -0.685244   \n",
            "1252    -0.685244    -0.685244    -0.685244    -0.685244     -0.685244   \n",
            "1253    -0.685244    -0.685244    -0.685244    -0.685244     -0.685244   \n",
            "1254    -0.685244    -0.685244    -0.685244    -0.685244     -0.685244   \n",
            "\n",
            "      mid_price  \n",
            "0     -1.477178  \n",
            "1     -1.477178  \n",
            "2     -1.477178  \n",
            "3     -1.477178  \n",
            "4     -1.477178  \n",
            "...         ...  \n",
            "1250  -0.689842  \n",
            "1251  -0.685244  \n",
            "1252  -0.685244  \n",
            "1253  -0.685244  \n",
            "1254  -0.685244  \n",
            "\n",
            "[1255 rows x 56 columns]\n",
            "\n",
            "Tahap 6: Membuat time-series sliding window...\n",
            "Data input: 1255 baris\n",
            "Window size: 10\n",
            "Jumlah fitur: 54\n",
            "Nama fitur: ['last_price', 'high_price', 'low_price', 'bid_price_1', 'offer_price_1']...\n",
            "Sliding window berhasil dibuat:\n",
            "- Jumlah sampel: 1245\n",
            "- Shape X: (1245, 10, 54) (samples, timesteps, features)\n",
            "- Shape y: (1245,)\n",
            "- Timestamps: 1245\n",
            "\n",
            "Tahap 7: Membagi data...\n",
            "Total sampel: 1245\n",
            "Pembagian data: Train=871, Val=186, Test=188\n",
            "\n",
            "Tahap 8: Format untuk Transformer...\n",
            "Data siap untuk model Transformer.\n",
            "\n",
            "==== RINGKASAN HASIL PREPROCESSING ====\n",
            "Data asli: 1315 baris\n",
            "Setelah sinkronisasi: 1255 baris\n",
            "Setelah sliding window: 1245 sampel\n",
            "Train: 871, Val: 186, Test: 188\n",
            "==== PREPROCESSING SELESAI ====\n",
            "\n",
            "Menyimpan hasil preprocessing sebagai CSV di /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO...\n",
            "Data hasil preprocessing tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/preprocessed_data.csv'\n",
            "Data train tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/train_data.csv' (871 baris)\n",
            "Data val tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/val_data.csv' (186 baris)\n",
            "Data test tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/test_data.csv' (188 baris)\n",
            "Metadata tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/metadata.json'\n",
            "Scaler tersimpan di '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/scaler.pkl'\n",
            "\n",
            "File CSV yang telah disimpan:\n",
            "- train_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/train_data.csv\n",
            "- val_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/val_data.csv\n",
            "- test_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/test_data.csv\n",
            "- preprocessed_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/preprocessed_data.csv\n",
            "- metadata_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/metadata.json\n",
            "- scaler_path: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO/scaler.pkl\n",
            "\n",
            "Preprocessing dan penyimpanan selesai.\n"
          ]
        }
      ],
      "source": [
        "# Install packages jika belum ada\n",
        "# !pip install pandas numpy matplotlib seaborn scikit-learn scipy torch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from scipy import stats\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# 1. Membaca dan Membersihkan Data Hilang dan Anomali\n",
        "def load_and_clean_data(file_path):\n",
        "    print(\"Tahap 1: Membaca dan membersihkan data...\")\n",
        "\n",
        "    df_raw = pd.read_csv(file_path)\n",
        "    print(f\"Data awal: {df_raw.shape[0]} baris, {df_raw.shape[1]} kolom\")\n",
        "    print(\"5 baris pertama:\")\n",
        "    print(df_raw.head())\n",
        "\n",
        "    # Periksa dan tampilkan baris yang seluruhnya NaN\n",
        "    nan_rows = df_raw[df_raw.isnull().all(axis=1)]\n",
        "    print(f\"Jumlah baris yang seluruhnya NaN: {len(nan_rows)}\")\n",
        "    if len(nan_rows) > 0:\n",
        "        print(\"Contoh baris yang seluruhnya NaN:\")\n",
        "        print(nan_rows.head())\n",
        "\n",
        "    # Baca ulang dengan skip_blank_lines=True untuk menghindari baris kosong\n",
        "    df = pd.read_csv(file_path, skip_blank_lines=True)\n",
        "    print(f\"Dataset awal memiliki {df.shape[0]} baris dan {df.shape[1]} kolom.\")\n",
        "\n",
        "    # Hapus baris yang seluruhnya NaN\n",
        "    df = df.dropna(how='all')\n",
        "    print(f\"Setelah menghapus baris kosong: {df.shape[0]} baris\")\n",
        "\n",
        "    # Konversi timestamp ke format datetime\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # Periksa nilai yang hilang di setiap kolom\n",
        "    missing_values = df.isnull().sum()\n",
        "    missing_cols = missing_values[missing_values > 0]\n",
        "    if len(missing_cols) > 0:\n",
        "        print(f\"Jumlah nilai yang hilang di setiap kolom:\\n{missing_cols}\")\n",
        "        # Interpolasi nilai yang hilang\n",
        "        df = df.interpolate(method='linear')\n",
        "        print(\"Nilai yang hilang telah diinterpolasi.\")\n",
        "    else:\n",
        "        print(\"Tidak ada nilai yang hilang dalam dataset.\")\n",
        "\n",
        "    # Deteksi dan tangani anomali pada kolom harga\n",
        "    price_columns = [col for col in df.columns if 'price' in col]\n",
        "    anomaly_count = 0\n",
        "    for col in price_columns:\n",
        "        if col in df.columns:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound))\n",
        "            anomaly_count += outliers.sum()\n",
        "            if outliers.sum() > 0:\n",
        "                print(f\"Ditemukan {outliers.sum()} anomali pada kolom {col}\")\n",
        "                df.loc[df[col] < lower_bound, col] = lower_bound\n",
        "                df.loc[df[col] > upper_bound, col] = upper_bound\n",
        "\n",
        "    if anomaly_count == 0:\n",
        "        print(\"Tidak ditemukan anomali dalam kolom harga.\")\n",
        "\n",
        "    print(f\"Dataset setelah dibersihkan memiliki {df.shape[0]} baris.\")\n",
        "    return df\n",
        "\n",
        "# 2. Sinkronisasi Waktu\n",
        "def synchronize_time(df, interval='1min'):\n",
        "    print(\"\\nTahap 2: Melakukan sinkronisasi waktu...\")\n",
        "    print(f\"Data sebelum resampling: {df.shape[0]} baris\")\n",
        "\n",
        "    df = df.set_index('timestamp')\n",
        "\n",
        "    price_cols = [col for col in df.columns if 'price' in col]\n",
        "    volume_cols = [col for col in df.columns if 'volume' in col]\n",
        "    agg_dict = {col: 'mean' for col in price_cols}\n",
        "    agg_dict.update({col: 'sum' for col in volume_cols})\n",
        "    for col in df.columns:\n",
        "        if col not in agg_dict:\n",
        "            agg_dict[col] = 'mean'\n",
        "\n",
        "    df_resampled = df.resample(interval).agg(agg_dict)\n",
        "\n",
        "    # Hapus baris yang memiliki NaN setelah resampling\n",
        "    df_resampled = df_resampled.dropna()\n",
        "    df_resampled.reset_index(inplace=True)\n",
        "\n",
        "    print(f\"Dataset setelah sinkronisasi waktu memiliki {df_resampled.shape[0]} baris.\")\n",
        "    return df_resampled\n",
        "\n",
        "# 3 & 4. Hitung Mid-Price\n",
        "def calculate_mid_price(df):\n",
        "    print(\"\\nTahap 3 & 4: Menghitung mid-price...\")\n",
        "\n",
        "    # Cari kolom bid dan offer yang tersedia\n",
        "    bid_cols = [col for col in df.columns if 'bid_price' in col]\n",
        "    offer_cols = [col for col in df.columns if 'offer_price' in col]\n",
        "\n",
        "    print(f\"Ditemukan {len(bid_cols)} kolom bid dan {len(offer_cols)} kolom offer\")\n",
        "\n",
        "    # Hitung mid-price untuk setiap level yang tersedia\n",
        "    mid_price_cols = []\n",
        "    for bid_col in bid_cols:\n",
        "        # Ekstrak level dari nama kolom\n",
        "        level = bid_col.split('_')[-1]\n",
        "        offer_col = f'offer_price_{level}'\n",
        "\n",
        "        if offer_col in df.columns:\n",
        "            mid_col = f'mid_price_{level}'\n",
        "            df[mid_col] = (df[bid_col] + df[offer_col]) / 2\n",
        "            mid_price_cols.append(mid_col)\n",
        "\n",
        "    # Hitung mid-price gabungan jika ada lebih dari satu level\n",
        "    if len(mid_price_cols) > 0:\n",
        "        df['mid_price'] = df[mid_price_cols].mean(axis=1)\n",
        "        print(f\"Mid-price telah ditambahkan untuk {len(mid_price_cols)} level dan mid-price gabungan.\")\n",
        "    else:\n",
        "        print(\"Warning: Tidak ditemukan pasangan bid-offer price untuk menghitung mid-price\")\n",
        "        # Jika tidak ada kolom price, buat mid_price dummy\n",
        "        df['mid_price'] = 0\n",
        "\n",
        "    return df\n",
        "\n",
        "# 5. Normalisasi dan Standarisasi\n",
        "def normalize_standardize_data(df, method='standardize'):\n",
        "    print(\"\\nTahap 5: Melakukan normalisasi dan standarisasi...\")\n",
        "    print(f\"Data sebelum scaling: {df.shape[0]} baris, {df.shape[1]} kolom\")\n",
        "\n",
        "    timestamp = df['timestamp'].copy()\n",
        "    features = df.drop('timestamp', axis=1)\n",
        "\n",
        "    if method == 'normalize':\n",
        "        scaler = MinMaxScaler()\n",
        "        features_scaled = scaler.fit_transform(features)\n",
        "        print(\"Data telah dinormalisasi ke rentang [0, 1].\")\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "        features_scaled = scaler.fit_transform(features)\n",
        "        print(\"Data telah distandarisasi dengan mean=0, std=1.\")\n",
        "\n",
        "    df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
        "    df_scaled['timestamp'] = timestamp.values\n",
        "    cols = df_scaled.columns.tolist()\n",
        "    df_scaled = df_scaled[['timestamp'] + [col for col in cols if col != 'timestamp']]\n",
        "\n",
        "    print(f\"Data setelah scaling: {df_scaled.shape[0]} baris, {df_scaled.shape[1]} kolom\")\n",
        "    return df_scaled, scaler\n",
        "\n",
        "# 6. Sliding Window - DIPERBAIKI\n",
        "def create_sliding_window(df, window_size=10):\n",
        "    print(\"\\nTahap 6: Membuat time-series sliding window...\")\n",
        "    print(f\"Data input: {df.shape[0]} baris\")\n",
        "    print(f\"Window size: {window_size}\")\n",
        "\n",
        "    # Pastikan kita memiliki cukup data untuk sliding window\n",
        "    if len(df) <= window_size:\n",
        "        raise ValueError(f\"Dataset terlalu kecil. Membutuhkan minimal {window_size + 1} baris, tetapi hanya memiliki {len(df)} baris.\")\n",
        "\n",
        "    # Simpan timestamp untuk referensi nanti\n",
        "    timestamps = df['timestamp'].values\n",
        "\n",
        "    # Ambil features (semua kolom kecuali timestamp dan mid_price)\n",
        "    feature_cols = [col for col in df.columns if col not in ['timestamp', 'mid_price']]\n",
        "    features = df[feature_cols].values\n",
        "    targets = df['mid_price'].values\n",
        "\n",
        "    print(f\"Jumlah fitur: {len(feature_cols)}\")\n",
        "    print(f\"Nama fitur: {feature_cols[:5]}...\")  # Tampilkan 5 fitur pertama\n",
        "\n",
        "    X, y, window_timestamps = [], [], []\n",
        "\n",
        "    # Membuat sliding window\n",
        "    for i in range(len(df) - window_size):\n",
        "        X.append(features[i:i+window_size])\n",
        "        y.append(targets[i+window_size])\n",
        "        window_timestamps.append(timestamps[i+window_size])  # Timestamp untuk target\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    window_timestamps = np.array(window_timestamps)\n",
        "\n",
        "    print(f\"Sliding window berhasil dibuat:\")\n",
        "    print(f\"- Jumlah sampel: {len(X)}\")\n",
        "    print(f\"- Shape X: {X.shape} (samples, timesteps, features)\")\n",
        "    print(f\"- Shape y: {y.shape}\")\n",
        "    print(f\"- Timestamps: {len(window_timestamps)}\")\n",
        "\n",
        "    return X, y, window_timestamps\n",
        "\n",
        "# 7. Split Data - DIPERBAIKI\n",
        "def split_data(X, y, timestamps, train_ratio=0.7, val_ratio=0.15):\n",
        "    print(\"\\nTahap 7: Membagi data...\")\n",
        "    n_samples = len(X)\n",
        "    print(f\"Total sampel: {n_samples}\")\n",
        "\n",
        "    if n_samples < 10:\n",
        "        print(\"Warning: Dataset sangat kecil, menyesuaikan rasio pembagian...\")\n",
        "        train_ratio = 0.6\n",
        "        val_ratio = 0.2\n",
        "\n",
        "    train_size = int(n_samples * train_ratio)\n",
        "    val_size = int(n_samples * val_ratio)\n",
        "\n",
        "    # Pastikan minimal ada 1 sampel untuk setiap split\n",
        "    train_size = max(1, train_size)\n",
        "    val_size = max(1, min(val_size, n_samples - train_size - 1))\n",
        "    test_size = n_samples - train_size - val_size\n",
        "\n",
        "    print(f\"Pembagian data: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
        "\n",
        "    X_train = X[:train_size]\n",
        "    y_train = y[:train_size]\n",
        "    ts_train = timestamps[:train_size]\n",
        "\n",
        "    X_val = X[train_size:train_size+val_size]\n",
        "    y_val = y[train_size:train_size+val_size]\n",
        "    ts_val = timestamps[train_size:train_size+val_size]\n",
        "\n",
        "    X_test = X[train_size+val_size:]\n",
        "    y_test = y[train_size+val_size:]\n",
        "    ts_test = timestamps[train_size+val_size:]\n",
        "\n",
        "    return (X_train, y_train, ts_train,\n",
        "            X_val, y_val, ts_val,\n",
        "            X_test, y_test, ts_test)\n",
        "\n",
        "# 8. Format for Transformer\n",
        "def format_for_transformer(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    print(\"\\nTahap 8: Format untuk Transformer...\")\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.FloatTensor(y_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.FloatTensor(y_val)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "    y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "    def get_positional_encoding(seq_len, d_model):\n",
        "        position = np.arange(seq_len)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "        pos_encoding = np.zeros((seq_len, d_model))\n",
        "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "        if d_model > 1:\n",
        "            pos_encoding[:, 1::2] = np.cos(position * div_term[:len(pos_encoding[0, 1::2])])\n",
        "        return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "    seq_len, feature_dim = X_train.shape[1], X_train.shape[2]\n",
        "    pos_encoding = get_positional_encoding(seq_len, feature_dim)\n",
        "\n",
        "    # Tambahkan positional encoding\n",
        "    for tensor in [X_train_tensor, X_val_tensor, X_test_tensor]:\n",
        "        for i in range(tensor.size(0)):\n",
        "            tensor[i] += pos_encoding\n",
        "\n",
        "    print(\"Data siap untuk model Transformer.\")\n",
        "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor\n",
        "\n",
        "# Utility\n",
        "def print_full_dataframe(df, info=None):\n",
        "    if info:\n",
        "        print(f\"\\n=== {info} ===\")\n",
        "    pd.set_option('display.max_rows', 20)  # Batasi tampilan untuk readability\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    print(df)\n",
        "    pd.reset_option('display.max_rows')\n",
        "    pd.reset_option('display.max_columns')\n",
        "\n",
        "# Fungsi untuk menyimpan hasil preprocessing dalam format CSV - DIPERBAIKI\n",
        "def save_preprocessed_data_as_csv(results, output_dir, include_timestamps=True):\n",
        "    \"\"\"\n",
        "    Menyimpan hasil preprocessing dalam format CSV.\n",
        "    \"\"\"\n",
        "    print(f\"\\nMenyimpan hasil preprocessing sebagai CSV di {output_dir}...\")\n",
        "\n",
        "    # Buat direktori jika belum ada\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 1. Simpan DataFrame hasil preprocessing lengkap\n",
        "    df_preprocessed = results['df_preprocessed']\n",
        "    preprocessed_path = os.path.join(output_dir, 'preprocessed_data.csv')\n",
        "    df_preprocessed.to_csv(preprocessed_path, index=False)\n",
        "    print(f\"Data hasil preprocessing tersimpan di '{preprocessed_path}'\")\n",
        "\n",
        "    # 2. Simpan data train, val, test dengan format yang lebih sederhana\n",
        "    datasets = [\n",
        "        ('train', results['X_train'], results['y_train'], results.get('ts_train')),\n",
        "        ('val', results['X_val'], results['y_val'], results.get('ts_val')),\n",
        "        ('test', results['X_test'], results['y_test'], results.get('ts_test'))\n",
        "    ]\n",
        "\n",
        "    file_paths = {}\n",
        "\n",
        "    for split_name, X, y, timestamps in datasets:\n",
        "        if len(X) == 0:\n",
        "            print(f\"Warning: {split_name} dataset kosong, melewati...\")\n",
        "            continue\n",
        "\n",
        "        # Konversi tensor ke numpy jika perlu\n",
        "        if torch.is_tensor(X):\n",
        "            X_np = X.cpu().numpy()\n",
        "            y_np = y.cpu().numpy()\n",
        "        else:\n",
        "            X_np = X\n",
        "            y_np = y\n",
        "\n",
        "        # Buat dataframe untuk split ini\n",
        "        data_dict = {}\n",
        "\n",
        "        # Tambahkan timestamp jika ada\n",
        "        if timestamps is not None and include_timestamps:\n",
        "            data_dict['timestamp'] = timestamps\n",
        "\n",
        "        # Tambahkan target\n",
        "        data_dict['target'] = y_np\n",
        "\n",
        "        # Flatten sliding window features dengan nama yang lebih sederhana\n",
        "        n_timesteps, n_features = X_np.shape[1], X_np.shape[2]\n",
        "\n",
        "        for t in range(n_timesteps):\n",
        "            for f in range(n_features):\n",
        "                feature_name = f't{t}_f{f}'\n",
        "                data_dict[feature_name] = X_np[:, t, f]\n",
        "\n",
        "        # Buat DataFrame dan simpan\n",
        "        df_split = pd.DataFrame(data_dict)\n",
        "        file_path = os.path.join(output_dir, f'{split_name}_data.csv')\n",
        "        df_split.to_csv(file_path, index=False)\n",
        "        file_paths[f'{split_name}_path'] = file_path\n",
        "        print(f\"Data {split_name} tersimpan di '{file_path}' ({len(df_split)} baris)\")\n",
        "\n",
        "    # 3. Simpan metadata\n",
        "    X_sample = results['X_train']\n",
        "    if torch.is_tensor(X_sample):\n",
        "        X_sample = X_sample.cpu().numpy()\n",
        "\n",
        "    metadata = {\n",
        "        'original_data_shape': results['df_original'].shape,\n",
        "        'preprocessed_data_shape': results['df_preprocessed'].shape,\n",
        "        'window_size': X_sample.shape[1] if len(X_sample) > 0 else 0,\n",
        "        'n_features': X_sample.shape[2] if len(X_sample) > 0 else 0,\n",
        "        'n_samples_train': len(results['X_train']),\n",
        "        'n_samples_val': len(results['X_val']),\n",
        "        'n_samples_test': len(results['X_test']),\n",
        "        'scaling_method': 'standardize' if isinstance(results['scaler'], StandardScaler) else 'normalize',\n",
        "        'feature_names': list(df_preprocessed.columns) if df_preprocessed is not None else None,\n",
        "        'timestamp_included': include_timestamps,\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    # Simpan metadata sebagai JSON\n",
        "    metadata_path = os.path.join(output_dir, 'metadata.json')\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "    print(f\"Metadata tersimpan di '{metadata_path}'\")\n",
        "\n",
        "    # Simpan scaler\n",
        "    scaler_path = os.path.join(output_dir, 'scaler.pkl')\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(results['scaler'], f)\n",
        "    print(f\"Scaler tersimpan di '{scaler_path}'\")\n",
        "\n",
        "    file_paths.update({\n",
        "        'preprocessed_path': preprocessed_path,\n",
        "        'metadata_path': metadata_path,\n",
        "        'scaler_path': scaler_path\n",
        "    })\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "# Pipeline - DIPERBAIKI\n",
        "def preprocess_lob_data(file_path, window_size=60, scaling_method='standardize', time_interval='1min', show_full_data=False):\n",
        "    print(\"==== MULAI PREPROCESSING DATASET LOB ====\")\n",
        "\n",
        "    # Tahap 1: Load and clean\n",
        "    df_clean = load_and_clean_data(file_path)\n",
        "    if show_full_data:\n",
        "        print_full_dataframe(df_clean, \"Data setelah dibersihkan\")\n",
        "\n",
        "    # Tahap 2: Synchronize time\n",
        "    df_sync = synchronize_time(df_clean, interval=time_interval)\n",
        "    if show_full_data:\n",
        "        print_full_dataframe(df_sync, \"Data setelah sinkronisasi waktu\")\n",
        "\n",
        "    # Tahap 3-4: Calculate mid-price\n",
        "    df_mid = calculate_mid_price(df_sync)\n",
        "    if show_full_data:\n",
        "        mid_price_cols = [col for col in df_mid.columns if 'mid_price' in col]\n",
        "        print_full_dataframe(df_mid[['timestamp'] + mid_price_cols], \"Kolom mid-price\")\n",
        "\n",
        "    # Tahap 5: Scale data\n",
        "    df_scaled, scaler = normalize_standardize_data(df_mid, method=scaling_method)\n",
        "    if show_full_data:\n",
        "        print_full_dataframe(df_scaled, \"Data setelah scaling\")\n",
        "\n",
        "    # Tahap 6: Create sliding window\n",
        "    X, y, timestamps = create_sliding_window(df_scaled, window_size=window_size)\n",
        "\n",
        "    # Tahap 7: Split data\n",
        "    (X_train, y_train, ts_train,\n",
        "     X_val, y_val, ts_val,\n",
        "     X_test, y_test, ts_test) = split_data(X, y, timestamps)\n",
        "\n",
        "    # Tahap 8: Format for transformer\n",
        "    X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor = format_for_transformer(\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test\n",
        "    )\n",
        "\n",
        "    print(\"\\n==== RINGKASAN HASIL PREPROCESSING ====\")\n",
        "    print(f\"Data asli: {df_clean.shape[0]} baris\")\n",
        "    print(f\"Setelah sinkronisasi: {df_sync.shape[0]} baris\")\n",
        "    print(f\"Setelah sliding window: {len(X)} sampel\")\n",
        "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "    print(\"==== PREPROCESSING SELESAI ====\")\n",
        "\n",
        "    return {\n",
        "        'df_original': df_clean,\n",
        "        'df_preprocessed': df_scaled,\n",
        "        'scaler': scaler,\n",
        "        'X_train': X_train_tensor,\n",
        "        'y_train': y_train_tensor,\n",
        "        'X_val': X_val_tensor,\n",
        "        'y_val': y_val_tensor,\n",
        "        'X_test': X_test_tensor,\n",
        "        'y_test': y_test_tensor,\n",
        "        'ts_train': ts_train,\n",
        "        'ts_val': ts_val,\n",
        "        'ts_test': ts_test\n",
        "    }\n",
        "\n",
        "# Pemanggilan jika dijalankan sebagai script utama\n",
        "if __name__ == \"__main__\":\n",
        "    # Untuk Google Colab\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # file_path = '/content/drive/MyDrive/SKRIPSI/Dataset_1/lob_data_BRPT.csv'\n",
        "\n",
        "    # Untuk local testing\n",
        "    file_path = '/content/drive/MyDrive/SKRIPSI/Dataset/lob_data_PTRO.csv'\n",
        "\n",
        "    # Jalankan preprocessing dengan window size yang lebih kecil untuk dataset kecil\n",
        "    results = preprocess_lob_data(\n",
        "        file_path=file_path,\n",
        "        window_size=10,  # Gunakan window size yang lebih kecil\n",
        "        scaling_method='standardize',\n",
        "        time_interval='1min',\n",
        "        show_full_data=True  # Ubah ke False jika tidak ingin melihat detail\n",
        "    )\n",
        "\n",
        "    # Simpan hasil preprocessing sebagai CSV\n",
        "    output_dir = '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_PTRO' # Sesuaikan path\n",
        "    csv_files = save_preprocessed_data_as_csv(results, output_dir)\n",
        "\n",
        "    # Informasi file yang telah disimpan\n",
        "    print(\"\\nFile CSV yang telah disimpan:\")\n",
        "    for file_type, path in csv_files.items():\n",
        "        print(f\"- {file_type}: {path}\")\n",
        "\n",
        "    print(\"\\nPreprocessing dan penyimpanan selesai.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgTlz-HRA7Ex"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ================================\n",
        "# 1. SIMPLIFIED DATA LOADER\n",
        "# ================================\n",
        "\n",
        "def load_preprocessed_csv_data(data_dir, debug=True):\n",
        "    \"\"\"\n",
        "    Simplified data loader focusing on core functionality\n",
        "    \"\"\"\n",
        "    print(f\"Loading preprocessed data from: {data_dir}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_path = os.path.join(data_dir, 'metadata.json')\n",
        "    if not os.path.exists(metadata_path):\n",
        "        raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n",
        "\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(f\"Metadata loaded: {list(metadata.keys())}\")\n",
        "\n",
        "    # Load scaler\n",
        "    scaler_path = os.path.join(data_dir, 'scaler.pkl')\n",
        "    scaler = None\n",
        "    if os.path.exists(scaler_path):\n",
        "        with open(scaler_path, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        print(\"Scaler loaded successfully\")\n",
        "\n",
        "    # Load data files with flexible naming\n",
        "    datasets = {}\n",
        "    file_mappings = {\n",
        "        'train': ['train_data.csv'],\n",
        "        'val': ['val_data.csv', 'validation_data.csv'],\n",
        "        'test': ['test_data.csv']\n",
        "    }\n",
        "\n",
        "    for split, possible_names in file_mappings.items():\n",
        "        found = False\n",
        "        for file_name in possible_names:\n",
        "            file_path = os.path.join(data_dir, file_name)\n",
        "            if os.path.exists(file_path):\n",
        "                datasets[split] = pd.read_csv(file_path)\n",
        "                print(f\"{split.capitalize()} data: {len(datasets[split])} samples from {file_name}\")\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            raise FileNotFoundError(f\"{split.capitalize()} data not found. Tried: {possible_names}\")\n",
        "\n",
        "    # Extract features and targets\n",
        "    def extract_features_targets(df, debug_name=\"\"):\n",
        "        # Find target column\n",
        "        target_col = None\n",
        "        if 'target' in df.columns:\n",
        "            target_col = 'target'\n",
        "        else:\n",
        "            target_candidates = [col for col in df.columns if any(x in col.lower() for x in ['mid_price', 'target', 'price'])]\n",
        "            if target_candidates:\n",
        "                target_col = target_candidates[0]\n",
        "                print(f\"Using '{target_col}' as target for {debug_name}\")\n",
        "\n",
        "        if target_col is None:\n",
        "            raise ValueError(f\"No target column found in {debug_name} data\")\n",
        "\n",
        "        y = df[target_col].values\n",
        "\n",
        "        # Extract features (look for time-feature pattern)\n",
        "        feature_pattern = re.compile(r't(\\d+)_f(\\d+)')\n",
        "        feature_cols = []\n",
        "        timesteps = set()\n",
        "        features = set()\n",
        "\n",
        "        for col in df.columns:\n",
        "            match = feature_pattern.search(col)\n",
        "            if match:\n",
        "                t, f = int(match.group(1)), int(match.group(2))\n",
        "                timesteps.add(t)\n",
        "                features.add(f)\n",
        "                feature_cols.append((col, t, f))\n",
        "\n",
        "        if not feature_cols:\n",
        "            # Fallback: use all non-target columns\n",
        "            exclude_cols = {'target', 'timestamp', 'index', target_col}\n",
        "            feature_cols = [(col, 0, i) for i, col in enumerate(df.columns)\n",
        "                           if col not in exclude_cols and not any(x in col.lower() for x in ['timestamp', 'index'])]\n",
        "            print(f\"Warning: No time-feature pattern found, using {len(feature_cols)} columns as features\")\n",
        "            timesteps = {0}\n",
        "            features = set(range(len(feature_cols)))\n",
        "\n",
        "        # Determine dimensions\n",
        "        n_timesteps = len(timesteps)\n",
        "        n_features = len(features)\n",
        "        n_samples = len(df)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"{debug_name} - Timesteps: {sorted(timesteps)}\")\n",
        "            print(f\"{debug_name} - Features: {sorted(features)}\")\n",
        "            print(f\"{debug_name} - Shape will be: ({n_samples}, {n_timesteps}, {n_features})\")\n",
        "\n",
        "        # Create 3D array\n",
        "        X = np.zeros((n_samples, n_timesteps, n_features))\n",
        "\n",
        "        # Fill array\n",
        "        for col_name, t, f in feature_cols:\n",
        "            if t in timesteps and f in features:\n",
        "                t_idx = sorted(timesteps).index(t)\n",
        "                f_idx = sorted(features).index(f)\n",
        "\n",
        "                # Ensure column exists and has valid data\n",
        "                if col_name in df.columns:\n",
        "                    col_data = df[col_name].values\n",
        "                    # Replace inf values with finite numbers\n",
        "                    col_data = np.where(np.isinf(col_data), 0, col_data)\n",
        "                    X[:, t_idx, f_idx] = col_data\n",
        "\n",
        "        # Handle NaN and inf values\n",
        "        X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        # Clip extreme values to prevent gradient issues\n",
        "        X = np.clip(X, -1e6, 1e6)\n",
        "        y = np.clip(y, -1e6, 1e6)\n",
        "\n",
        "        # Clean NaN/inf in targets\n",
        "        if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n",
        "            print(f\"Warning: Cleaning NaN/inf in {debug_name} targets\")\n",
        "            y = np.nan_to_num(y, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        return X, y, n_timesteps, n_features\n",
        "\n",
        "    # Process all datasets\n",
        "    X_train, y_train, n_timesteps, n_features = extract_features_targets(datasets['train'], \"train\")\n",
        "    X_val, y_val, _, _ = extract_features_targets(datasets['val'], \"validation\")\n",
        "    X_test, y_test, _, _ = extract_features_targets(datasets['test'], \"test\")\n",
        "\n",
        "    # Data validation\n",
        "    print(f\"\\nData validation:\")\n",
        "    print(f\"Train: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"Val:   X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"Test:  X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    data = {\n",
        "        'X_train': torch.FloatTensor(X_train),\n",
        "        'y_train': torch.FloatTensor(y_train),\n",
        "        'X_val': torch.FloatTensor(X_val),\n",
        "        'y_val': torch.FloatTensor(y_val),\n",
        "        'X_test': torch.FloatTensor(X_test),\n",
        "        'y_test': torch.FloatTensor(y_test),\n",
        "        'scaler': scaler,\n",
        "        'n_timesteps': n_timesteps,\n",
        "        'n_features': n_features\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "# ================================\n",
        "# 2. SIMPLIFIED TRANSFORMER MODEL\n",
        "# ================================\n",
        "\n",
        "class SimplePositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard positional encoding\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=1000, dropout=0.1):\n",
        "        super(SimplePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return self.dropout(x + self.pe[:, :seq_len, :])\n",
        "\n",
        "class SimpleLOBTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified LOB Transformer focusing on core performance\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, n_heads=8, n_layers=4, d_ff=512,\n",
        "                 dropout=0.1, max_seq_len=100):\n",
        "        super(SimpleLOBTransformer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input processing\n",
        "        self.input_norm = nn.LayerNorm(input_dim)\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = SimplePositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Standard transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Output layers\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, d_model // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 4, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input processing\n",
        "        x = self.input_norm(x)\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = x.transpose(1, 2)  # [batch, features, seq_len]\n",
        "        x = self.global_pool(x).squeeze(-1)  # [batch, features]\n",
        "\n",
        "        # Output prediction\n",
        "        output = self.output_head(x).squeeze(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ================================\n",
        "# 3. SIMPLIFIED TRAINER\n",
        "# ================================\n",
        "\n",
        "class SimpleTrainer:\n",
        "    \"\"\"\n",
        "    Simplified training system focusing on core metrics\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_loader, val_loader, test_loader, config):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.config = config\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config['learning_rate'],\n",
        "            weight_decay=config['weight_decay']\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        for batch_idx, (X, y) in enumerate(self.train_loader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Skip problematic batches\n",
        "            if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                continue\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(X)\n",
        "\n",
        "            # Skip if output contains NaN\n",
        "            if torch.isnan(outputs).any():\n",
        "                continue\n",
        "\n",
        "            loss = self.criterion(outputs, y)\n",
        "\n",
        "            # Skip if loss is NaN\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "\n",
        "            # Backward pass\n",
        "            try:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                all_preds.extend(outputs.detach().cpu().numpy())\n",
        "                all_targets.extend(y.detach().cpu().numpy())\n",
        "                valid_batches += 1\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Skipping batch {batch_idx} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in self.val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(X)\n",
        "\n",
        "                if torch.isnan(outputs).any():\n",
        "                    continue\n",
        "\n",
        "                loss = self.criterion(outputs, y)\n",
        "\n",
        "                if torch.isnan(loss):\n",
        "                    continue\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(y.cpu().numpy())\n",
        "                valid_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Starting training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config['num_epochs']):\n",
        "            # Training\n",
        "            train_loss, train_preds, train_targets = self.train_epoch()\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_preds, val_targets = self.validate()\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Calculate R for monitoring\n",
        "            train_r2 = r2_score(train_targets, train_preds) if len(train_targets) > 0 else 0\n",
        "            val_r2 = r2_score(val_targets, val_preds) if len(val_targets) > 0 else 0\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch+1:3d}/{self.config['num_epochs']} | \"\n",
        "                  f\"Train Loss: {train_loss:.6f} | Train R: {train_r2:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.6f} | Val R: {val_r2:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.config['patience']:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        if self.best_model_state is not None:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"Training completed in {training_time/60:.2f} minutes\")\n",
        "\n",
        "        return self.final_evaluation()\n",
        "\n",
        "    def evaluate_loader(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(X)\n",
        "\n",
        "                if torch.isnan(outputs).any():\n",
        "                    continue\n",
        "\n",
        "                loss = self.criterion(outputs, y)\n",
        "\n",
        "                if not torch.isnan(loss):\n",
        "                    total_loss += loss.item()\n",
        "                    valid_batches += 1\n",
        "\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def calculate_core_metrics(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate only core metrics: MSE, RMSE, MAE, R\n",
        "        \"\"\"\n",
        "        if len(y_true) == 0 or len(y_pred) == 0:\n",
        "            return {'MSE': float('nan'), 'RMSE': float('nan'), 'MAE': float('nan'), 'R2': float('nan')}\n",
        "\n",
        "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        acc = regression_accuracy(y_true, y_pred, tolerance=0.02)\n",
        "\n",
        "        return {\n",
        "            'targets': y_true,\n",
        "            'predictions': y_pred,\n",
        "            'metrics': {\n",
        "                'MSE': mse,\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'R2': r2,\n",
        "                'Accuracy': acc\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "    def final_evaluation(self):\n",
        "        \"\"\"\n",
        "        Final evaluation with core metrics only\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FINAL EVALUATION - CORE METRICS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = {}\n",
        "        for name, loader in [('train', self.train_loader), ('val', self.val_loader), ('test', self.test_loader)]:\n",
        "            loss, preds, targets = self.evaluate_loader(loader)\n",
        "            metrics_result = self.calculate_core_metrics(targets, preds)\n",
        "            metrics = metrics_result['metrics']\n",
        "\n",
        "            results[name] = {\n",
        "                'loss': loss,\n",
        "                'predictions': preds,\n",
        "                'targets': targets,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "\n",
        "            print(f\"{name.capitalize():5} | MSE: {metrics['MSE']:.6f} | RMSE: {metrics['RMSE']:.6f} | \"\n",
        "                  f\"MAE: {metrics['MAE']:.6f} | R: {metrics['R2']:.4f} | Accuracy: {metrics['Accuracy']:.2%}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "def print_formatted_metrics(results):\n",
        "    \"\"\"\n",
        "    Format dan tampilkan metrik evaluasi model dalam bentuk tabel rapi\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*66)\n",
        "    print(\"FINAL METRICS SUMMARY\")\n",
        "    print(\"=\"*66)\n",
        "    print(f\"{'Dataset':<10} | {'MSE':>8} | {'RMSE':>8} | {'MAE':>8} | {'R':>8} | {'Accuracy':>9}\")\n",
        "    print(\"-\" * 66)\n",
        "\n",
        "    for name in ['train', 'val', 'test']:\n",
        "        metrics = results[name]['metrics']\n",
        "        mse = metrics['MSE']\n",
        "        rmse = metrics['RMSE']\n",
        "        mae = metrics['MAE']\n",
        "        r2 = metrics['R2']\n",
        "        acc = metrics['Accuracy']\n",
        "\n",
        "        print(f\"{name.capitalize():<10} | {mse:8.5f} | {rmse:8.4f} | {mae:8.4f} | {r2:8.4f} | {acc*100:8.2f}%\")\n",
        "\n",
        "    print(\"=\"*66)\n",
        "\n",
        "# ================================\n",
        "# 4. SIMPLE VISUALIZATION\n",
        "# ================================\n",
        "\n",
        "# def create_simple_visualizations(trainer, results, save_dir=None):\n",
        "#     \"\"\"\n",
        "#     Create simple visualizations focusing on core metrics\n",
        "#     \"\"\"\n",
        "#     # 1. Training curves\n",
        "#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "#     fig.suptitle('Training Progress', fontsize=14, fontweight='bold')\n",
        "\n",
        "#     epochs = range(1, len(trainer.train_losses) + 1)\n",
        "\n",
        "#     # Loss curves\n",
        "#     axes[0].plot(epochs, trainer.train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "#     axes[0].plot(epochs, trainer.val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "#     axes[0].set_title('Loss Curves')\n",
        "#     axes[0].set_xlabel('Epoch')\n",
        "#     axes[0].set_ylabel('Loss')\n",
        "#     axes[0].legend()\n",
        "#     axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "#     # Core metrics comparison\n",
        "#     splits = ['Train', 'Val', 'Test']\n",
        "#     r2_scores = [results[s.lower()]['metrics']['R2'] for s in splits]\n",
        "\n",
        "#     bars = axes[1].bar(splits, r2_scores, color=['blue', 'orange', 'red'], alpha=0.7)\n",
        "#     axes[1].set_title('R Scores')\n",
        "#     axes[1].set_ylabel('R Score')\n",
        "#     axes[1].set_ylim(0, 1)\n",
        "\n",
        "#     for bar, score in zip(bars, r2_scores):\n",
        "#         axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "#                     f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     if save_dir:\n",
        "#         plt.savefig(os.path.join(save_dir, 'training_results.png'), dpi=300, bbox_inches='tight')\n",
        "#     # plt.show()\n",
        "#     plt.close()\n",
        "\n",
        "#     # 2. Predictions scatter plot\n",
        "#     fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "#     fig.suptitle('Predictions vs Actual', fontsize=14, fontweight='bold')\n",
        "\n",
        "#     for i, (split, data) in enumerate(results.items()):\n",
        "#         targets = np.array(data['targets'])\n",
        "#         predictions = np.array(data['predictions'])\n",
        "\n",
        "#         if len(targets) == 0:\n",
        "#             continue\n",
        "\n",
        "#         axes[i].scatter(targets, predictions, alpha=0.6, s=20)\n",
        "#         min_val, max_val = min(targets.min(), predictions.min()), max(targets.max(), predictions.max())\n",
        "#         axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "#         axes[i].set_xlabel('Actual')\n",
        "#         axes[i].set_ylabel('Predicted')\n",
        "#         axes[i].set_title(f'{split.capitalize()} (R={data[\"metrics\"][\"R2\"]:.3f})')\n",
        "#         axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     if save_dir:\n",
        "#         plt.savefig(os.path.join(save_dir, 'predictions_scatter.png'), dpi=300, bbox_inches='tight')\n",
        "#     # plt.show()\n",
        "#     plt.close()\n",
        "\n",
        "def create_simple_visualizations(trainer, results, save_dir=None):\n",
        "    \"\"\"\n",
        "    Create and save simple visualizations focusing on core metrics.\n",
        "    This function avoids display and directly saves images.\n",
        "    \"\"\"\n",
        "    if save_dir is None:\n",
        "        print(\"[] Save directory not provided. Skipping visualizations.\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    plt.ioff()  # Turn off interactive mode\n",
        "\n",
        "    # === 1. Training Loss & R Bar Plot ===\n",
        "    # Visualisasi Loss\n",
        "    try:\n",
        "        fig1 = plt.figure(figsize=(6, 4), dpi=100)\n",
        "        plt.plot(trainer.train_losses, label='Train Loss', linewidth=2)\n",
        "        plt.plot(trainer.val_losses, label='Val Loss', linewidth=2)\n",
        "        plt.title('Training & Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        save_path1 = os.path.join(save_dir, 'loss_plot.png')\n",
        "        fig1.savefig(save_path1, bbox_inches='tight')\n",
        "        print(f\"[] Saved loss plot to: {save_path1}\")\n",
        "        plt.close(fig1)\n",
        "    except Exception as e:\n",
        "        print(f\"[] Failed to save loss plot: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "    # Visualisasi R\n",
        "    try:\n",
        "        fig2 = plt.figure(figsize=(5, 4), dpi=100)\n",
        "        r2_scores = [results[s.lower()]['metrics']['R2'] for s in ['Train', 'Val', 'Test']]\n",
        "        plt.bar(['Train', 'Val', 'Test'], r2_scores, color=['blue', 'orange', 'red'], alpha=0.7)\n",
        "        for i, score in enumerate(r2_scores):\n",
        "            plt.text(i, score + 0.01, f'{score:.3f}', ha='center', fontweight='bold')\n",
        "        plt.title('R Scores')\n",
        "        plt.ylim(min(0, min(r2_scores)), 1)\n",
        "        plt.ylabel('R Score')\n",
        "        save_path2 = os.path.join(save_dir, 'r2_scores.png')\n",
        "        fig2.savefig(save_path2, bbox_inches='tight')\n",
        "        print(f\"[] Saved R plot to: {save_path2}\")\n",
        "        plt.close(fig2)\n",
        "    except Exception as e:\n",
        "        print(f\"[] Failed to save R plot: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    # === 2. Predictions vs Actual Scatter Plots ===\n",
        "    try:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        fig.suptitle('Predictions vs Actual', fontsize=14, fontweight='bold')\n",
        "\n",
        "        for i, (split, data) in enumerate(results.items()):\n",
        "            targets = np.array(data['targets'])\n",
        "            predictions = np.array(data['predictions'])\n",
        "\n",
        "            if len(targets) == 0 or len(predictions) == 0:\n",
        "                axes[i].set_title(f\"{split.capitalize()} (No Data)\")\n",
        "                axes[i].axis('off')\n",
        "                continue\n",
        "\n",
        "            axes[i].scatter(targets, predictions, alpha=0.6, s=20)\n",
        "            min_val = min(targets.min(), predictions.min())\n",
        "            max_val = max(targets.max(), predictions.max())\n",
        "            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "            axes[i].set_xlabel('Actual')\n",
        "            axes[i].set_ylabel('Predicted')\n",
        "            axes[i].set_title(f'{split.capitalize()} (R={data[\"metrics\"][\"R2\"]:.3f})')\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path2 = os.path.join(save_dir, 'predictions_scatter.png')\n",
        "        plt.savefig(save_path2, dpi=300, bbox_inches='tight')\n",
        "        print(f\"[] Saved prediction scatter plot to: {save_path2}\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[] Failed to create/save scatter plot: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. CONFIGURATION\n",
        "# ================================\n",
        "\n",
        "def get_simple_config(data_info, risk_level='medium'):\n",
        "    \"\"\"\n",
        "    Get simple configuration based on data characteristics\n",
        "    \"\"\"\n",
        "    n_samples = data_info.get('n_train_samples', 1000)\n",
        "    n_features = data_info.get('n_features', 10)\n",
        "    n_timesteps = data_info.get('n_timesteps', 10)\n",
        "\n",
        "    total_features = n_features * n_timesteps\n",
        "    sample_feature_ratio = n_samples / total_features\n",
        "\n",
        "    print(f\"Data analysis:\")\n",
        "    print(f\"  Samples: {n_samples}\")\n",
        "    print(f\"  Features per timestep: {n_features}\")\n",
        "    print(f\"  Timesteps: {n_timesteps}\")\n",
        "    print(f\"  Sample/Feature ratio: {sample_feature_ratio:.2f}\")\n",
        "\n",
        "    # Simple configurations\n",
        "    configs = {\n",
        "        'low': {\n",
        "            'd_model': 256,\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 6,\n",
        "            'd_ff': 1024,\n",
        "            'dropout': 0.5,\n",
        "            'learning_rate': 0.001,\n",
        "            'weight_decay': 1e-5,\n",
        "            'batch_size': 32,\n",
        "            'num_epochs': 15,\n",
        "            'patience': 5\n",
        "        },\n",
        "        'medium': {\n",
        "            'd_model': 128,\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'd_ff': 512,\n",
        "            'dropout': 0.5,\n",
        "            'learning_rate': 0.0005,\n",
        "            'weight_decay': 1e-4,\n",
        "            'batch_size': 16,\n",
        "            'num_epochs': 15,\n",
        "            'patience': 5\n",
        "        },\n",
        "        'high': {\n",
        "           'd_model': 32,\n",
        "            'n_heads': 2,\n",
        "            'n_layers': 2,\n",
        "            'd_ff': 128,\n",
        "            'dropout': 0.5,\n",
        "            'learning_rate': 0.0001,\n",
        "            'weight_decay': 1e-3,\n",
        "            'batch_size': 8,\n",
        "            'num_epochs': 30,\n",
        "            'patience': 5\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Auto-detect risk level\n",
        "    if sample_feature_ratio < 5:\n",
        "        auto_risk = 'high'\n",
        "    elif sample_feature_ratio < 15:\n",
        "        auto_risk = 'medium'\n",
        "    else:\n",
        "        auto_risk = 'low'\n",
        "\n",
        "    if risk_level == 'auto':\n",
        "        risk_level = auto_risk\n",
        "\n",
        "    config = configs[risk_level].copy()\n",
        "    config['batch_size'] = min(config['batch_size'], max(4, n_samples // 10))\n",
        "\n",
        "    print(f\"Using '{risk_level}' risk configuration\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# ================================\n",
        "# 6. MAIN PIPELINE\n",
        "# ================================\n",
        "\n",
        "def run_simple_lob_transformer(data_dir, risk_level='auto', save_dir=None):\n",
        "    \"\"\"\n",
        "    Main pipeline for simple LOB transformer training\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"SIMPLE LOB TRANSFORMER - CORE METRICS ONLY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create save directory\n",
        "    if save_dir is None:\n",
        "        save_dir = '/kaggle/working/dataset_setelah_modeling_no_dropout/TPIA'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"\\n1. Loading data...\")\n",
        "    data = load_preprocessed_csv_data(data_dir, debug=True)\n",
        "\n",
        "    # 2. Get configuration\n",
        "    print(\"\\n2. Configuration...\")\n",
        "    data_info = {\n",
        "        'n_train_samples': len(data['X_train']),\n",
        "        'n_features': data['n_features'],\n",
        "        'n_timesteps': data['n_timesteps']\n",
        "    }\n",
        "\n",
        "    config = get_simple_config(data_info, risk_level)\n",
        "\n",
        "    print(f\"\\nFinal configuration:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    print(\"\\n3. Creating data loaders...\")\n",
        "    train_dataset = TensorDataset(data['X_train'], data['y_train'])\n",
        "    val_dataset = TensorDataset(data['X_val'], data['y_val'])\n",
        "    test_dataset = TensorDataset(data['X_test'], data['y_test'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    print(f\"  Train batches: {len(train_loader)}\")\n",
        "    print(f\"  Val batches: {len(val_loader)}\")\n",
        "    print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "    # 4. Create model\n",
        "    print(\"\\n4. Creating model...\")\n",
        "    model = SimpleLOBTransformer(\n",
        "        input_dim=data['n_features'],\n",
        "        d_model=config['d_model'],\n",
        "        n_heads=config['n_heads'],\n",
        "        n_layers=config['n_layers'],\n",
        "        d_ff=config['d_ff'],\n",
        "        dropout=config['dropout'],\n",
        "        max_seq_len=data['n_timesteps']\n",
        "    )\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "    # 5. Train model\n",
        "    print(\"\\n5. Training...\")\n",
        "    trainer = SimpleTrainer(model, train_loader, val_loader, test_loader, config)\n",
        "    results = trainer.train()\n",
        "    print_formatted_metrics(results)\n",
        "\n",
        "\n",
        "    # 6. Save model\n",
        "    print(\"\\n6. Saving model...\")\n",
        "    model_path = os.path.join(save_dir, 'simple_lob_transformer.pth')\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'data_info': data_info,\n",
        "        'model_architecture': {\n",
        "            'input_dim': data['n_features'],\n",
        "            'd_model': config['d_model'],\n",
        "            'n_heads': config['n_heads'],\n",
        "            'n_layers': config['n_layers'],\n",
        "            'd_ff': config['d_ff'],\n",
        "            'dropout': config['dropout'],\n",
        "            'max_seq_len': data['n_timesteps']\n",
        "        }\n",
        "    }, model_path)\n",
        "\n",
        "    if data['scaler'] is not None:\n",
        "        scaler_path = os.path.join(save_dir, 'model_scaler.pkl')\n",
        "        with open(scaler_path, 'wb') as f:\n",
        "            pickle.dump(data['scaler'], f)\n",
        "\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "    # 7. Create visualizations\n",
        "    print(\"\\n7. Creating visualizations...\")\n",
        "    create_simple_visualizations(trainer, results, save_dir)\n",
        "\n",
        "    # 8. Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    test_metrics = results['test']['metrics']\n",
        "    print(f\"\\nTest Set Performance:\")\n",
        "    print(f\"  MSE:  {test_metrics['MSE']:.6f}\")\n",
        "    print(f\"  RMSE: {test_metrics['RMSE']:.6f}\")\n",
        "    print(f\"  MAE:  {test_metrics['MAE']:.6f}\")\n",
        "    print(f\"  R:   {test_metrics['R2']:.4f}\")\n",
        "    print(f\"Accuracy: {test_metrics['Accuracy']:.2%}\")  # Ini oke, karena .2% akan dikali 100\n",
        "\n",
        "\n",
        "    # Overfitting analysis\n",
        "    train_r2 = results['train']['metrics']['R2']\n",
        "    val_r2 = results['val']['metrics']['R2']\n",
        "    test_r2 = results['test']['metrics']['R2']\n",
        "\n",
        "    print(f\"\\nGeneralization Analysis:\")\n",
        "    print(f\"  Train R: {train_r2:.4f}\")\n",
        "    print(f\"  Val R:   {val_r2:.4f}\")\n",
        "    print(f\"  Test R:  {test_r2:.4f}\")\n",
        "\n",
        "    overfitting_gap = train_r2 - val_r2\n",
        "    if overfitting_gap > 0.1:\n",
        "        print(f\"    Overfitting detected (gap: {overfitting_gap:.4f})\")\n",
        "    elif overfitting_gap < -0.05:\n",
        "        print(f\"    Underfitting detected (gap: {overfitting_gap:.4f})\")\n",
        "    else:\n",
        "        print(f\"   Good generalization (gap: {overfitting_gap:.4f})\")\n",
        "\n",
        "    return model, trainer, results\n",
        "\n",
        "# ================================\n",
        "# 7. PREDICTION INTERFACE\n",
        "# ================================\n",
        "\n",
        "class SimpleLOBPredictor:\n",
        "    \"\"\"\n",
        "    Simple prediction interface for the LOB model\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, scaler_path=None):\n",
        "        # Load model\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "        # Get model architecture\n",
        "        arch = checkpoint['model_architecture']\n",
        "        self.model = SimpleLOBTransformer(\n",
        "            input_dim=arch['input_dim'],\n",
        "            d_model=arch['d_model'],\n",
        "            n_heads=arch['n_heads'],\n",
        "            n_layers=arch['n_layers'],\n",
        "            d_ff=arch['d_ff'],\n",
        "            dropout=arch['dropout'],\n",
        "            max_seq_len=arch['max_seq_len']\n",
        "        )\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Load scaler\n",
        "        self.scaler = None\n",
        "        if scaler_path and os.path.exists(scaler_path):\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "\n",
        "        self.config = checkpoint['config']\n",
        "        print(f\"Model loaded successfully!\")\n",
        "        print(f\"Input shape expected: (batch_size, {arch['max_seq_len']}, {arch['input_dim']})\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions on new data\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(X):\n",
        "            X = torch.FloatTensor(X)\n",
        "\n",
        "        if len(X.shape) == 2:\n",
        "            X = X.unsqueeze(0)\n",
        "\n",
        "        X = X.to(device)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            prediction = self.model(X)\n",
        "            return prediction.cpu().numpy().squeeze()\n",
        "\n",
        "    def predict_next_price(self, lob_sequence):\n",
        "        \"\"\"\n",
        "        Predict the next mid-price given a LOB sequence\n",
        "        \"\"\"\n",
        "        prediction = self.predict(lob_sequence)\n",
        "        return float(prediction)\n",
        "\n",
        "# ================================\n",
        "# 8. UTILITY FUNCTIONS\n",
        "# ================================\n",
        "\n",
        "def compare_models_performance(results_dict):\n",
        "    \"\"\"\n",
        "    Compare performance of multiple models\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    metrics_df = pd.DataFrame()\n",
        "\n",
        "    for model_name, results in results_dict.items():\n",
        "        test_metrics = results['test']['metrics']\n",
        "        metrics_df[model_name] = [\n",
        "            test_metrics['MSE'],\n",
        "            test_metrics['RMSE'],\n",
        "            test_metrics['MAE'],\n",
        "            test_metrics['R2'],\n",
        "            test_metrics['Accuracy']\n",
        "        ]\n",
        "\n",
        "    metrics_df.index = ['MSE', 'RMSE', 'MAE', 'R', 'Accuracy']\n",
        "    print(metrics_df.round(6))\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "def save_results_to_csv(results, save_path):\n",
        "    \"\"\"\n",
        "    Save predictions and targets to CSV for further analysis\n",
        "    \"\"\"\n",
        "    test_data = results['test']\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'actual': test_data['targets'],\n",
        "        'predicted': test_data['predictions'],\n",
        "        'error': np.array(test_data['targets']) - np.array(test_data['predictions'])\n",
        "    })\n",
        "\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "\n",
        "def calculate_percentage_improvement(baseline_r2, new_r2):\n",
        "    \"\"\"\n",
        "    Calculate percentage improvement in R score\n",
        "    \"\"\"\n",
        "    if baseline_r2 <= 0:\n",
        "        return float('inf') if new_r2 > 0 else 0\n",
        "\n",
        "    improvement = ((new_r2 - baseline_r2) / abs(baseline_r2)) * 100\n",
        "    return improvement\n",
        "\n",
        "\n",
        "def regression_accuracy(y_true, y_pred, tolerance=0.2, min_absolute_tolerance=0.1):\n",
        "    y_true = np.array(y_true).squeeze()\n",
        "    y_pred = np.array(y_pred).squeeze()\n",
        "\n",
        "    # Pastikan 1D array\n",
        "    if y_true.ndim != 1 or y_pred.ndim != 1:\n",
        "        y_true = y_true.reshape(-1)\n",
        "        y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    # Filter NaN dan Inf\n",
        "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred) & ~np.isinf(y_true) & ~np.isinf(y_pred)\n",
        "    y_true = y_true[mask]\n",
        "    y_pred = y_pred[mask]\n",
        "\n",
        "    if len(y_true) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Hitung toleransi\n",
        "    tolerance_values = np.maximum(tolerance * np.abs(y_true), min_absolute_tolerance)\n",
        "    absolute_errors = np.abs(y_pred - y_true)\n",
        "\n",
        "    # Validasi ukuran array\n",
        "    if tolerance_values.shape != absolute_errors.shape:\n",
        "        print(\"WARNING: Shape mismatch after broadcasting\")\n",
        "        print(\"absolute_errors.shape:\", absolute_errors.shape)\n",
        "        print(\"tolerance_values.shape:\", tolerance_values.shape)\n",
        "\n",
        "    correct = absolute_errors <= tolerance_values\n",
        "    accuracy = np.mean(correct)  # Persen\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 9. BATCH EVALUATION\n",
        "# ================================\n",
        "\n",
        "def evaluate_multiple_configurations(data_dir, configs_to_test=None, save_dir=None):\n",
        "    \"\"\"\n",
        "    Test multiple configurations and return best performing one\n",
        "    \"\"\"\n",
        "    if configs_to_test is None:\n",
        "        configs_to_test = ['low', 'medium', 'high']\n",
        "\n",
        "    if save_dir is None:\n",
        "        save_dir = './model_comparison'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    results_dict = {}\n",
        "\n",
        "    for risk_level in configs_to_test:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TESTING CONFIGURATION: {risk_level.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            model_save_dir = os.path.join(save_dir, f'model_{risk_level}')\n",
        "            model, trainer, results = run_simple_lob_transformer(\n",
        "                data_dir=data_dir,\n",
        "                risk_level=risk_level,\n",
        "                save_dir=model_save_dir\n",
        "            )\n",
        "            results_dict[risk_level] = results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {risk_level} configuration: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Compare results\n",
        "    if results_dict:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"CONFIGURATION COMPARISON\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        comparison_df = compare_models_performance(results_dict)\n",
        "\n",
        "        # Find best configuration\n",
        "        best_config = comparison_df.loc['R'].idxmax()\n",
        "        best_r2 = comparison_df.loc['R'].max()\n",
        "\n",
        "        print(f\"\\nBest Configuration: {best_config}\")\n",
        "        print(f\"Best R Score: {best_r2:.4f}\")\n",
        "\n",
        "        # Save comparison\n",
        "        comparison_df.to_csv(os.path.join(save_dir, 'configuration_comparison.csv'))\n",
        "\n",
        "        return results_dict, best_config\n",
        "    else:\n",
        "        print(\"No successful configurations to compare!\")\n",
        "        return {}, None\n",
        "\n",
        "# ================================\n",
        "# 10. EXAMPLE USAGE\n",
        "# ================================\n",
        "\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the simplified LOB transformer\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"EXAMPLE USAGE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    example_code = '''\n",
        "    # 1. Simple training\n",
        "    model, trainer, results = run_simple_lob_transformer(\n",
        "        data_dir='/path/to/preprocessed_data',\n",
        "        risk_level='auto',  # or 'low', 'medium', 'high'\n",
        "        save_dir='/path/to/save/model'\n",
        "    )\n",
        "\n",
        "    # 2. View core metrics\n",
        "    test_metrics = results['test']['metrics']\n",
        "    print(f\"MSE: {test_metrics['MSE']:.6f}\")\n",
        "    print(f\"RMSE: {test_metrics['RMSE']:.6f}\")\n",
        "    print(f\"MAE: {test_metrics['MAE']:.6f}\")\n",
        "    print(f\"R: {test_metrics['R2']:.4f}\")\n",
        "\n",
        "    # 3. Make predictions\n",
        "    predictor = SimpleLOBPredictor(\n",
        "        model_path='/path/to/save/model/simple_lob_transformer.pth',\n",
        "        scaler_path='/path/to/save/model/model_scaler.pkl'\n",
        "    )\n",
        "\n",
        "    # Predict on new data\n",
        "    next_price = predictor.predict_next_price(new_lob_sequence)\n",
        "    print(f\"Predicted next mid-price: {next_price}\")\n",
        "\n",
        "    # 4. Test multiple configurations\n",
        "    results_dict, best_config = evaluate_multiple_configurations(\n",
        "        data_dir='/path/to/preprocessed_data',\n",
        "        configs_to_test=['low', 'medium', 'high']\n",
        "    )\n",
        "\n",
        "    # 5. Save results for analysis\n",
        "    save_results_to_csv(results['test'], '/path/to/predictions.csv')\n",
        "    '''\n",
        "\n",
        "    print(example_code)\n",
        "\n",
        "# ================================\n",
        "# 11. MAIN EXECUTION\n",
        "# ================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Simplified LOB Transformer Module Loaded!\")\n",
        "\n",
        "    # Show example usage\n",
        "    # example_usage()\n",
        "\n",
        "    # For direct usage, uncomment and modify the path:\n",
        "    data_dir = '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_TPIA'\n",
        "\n",
        "    # Single model training\n",
        "    model, trainer, results = run_simple_lob_transformer(\n",
        "        data_dir=data_dir,\n",
        "        risk_level='auto'\n",
        "    )\n",
        "\n",
        "    # Display final metrics clearly\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CORE METRICS SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    test_metrics = results['test']['metrics']\n",
        "    print(f\"MSE:  {test_metrics['MSE']:.6f}\")\n",
        "    print(f\"RMSE: {test_metrics['RMSE']:.6f}\")\n",
        "    print(f\"MAE:  {test_metrics['MAE']:.6f}\")\n",
        "    print(f\"R:   {test_metrics['R2']:.4f}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGQIXRWVLnjY",
        "outputId": "9052f6a4-791f-499f-b011-dbeae1fa5db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Simplified LOB Transformer Module Loaded!\n",
            "================================================================================\n",
            "SIMPLE LOB TRANSFORMER - CORE METRICS ONLY\n",
            "================================================================================\n",
            "\n",
            "1. Loading data...\n",
            "Loading preprocessed data from: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_TPIA\n",
            "Metadata loaded: ['original_data_shape', 'preprocessed_data_shape', 'window_size', 'n_features', 'n_samples_train', 'n_samples_val', 'n_samples_test', 'scaling_method', 'feature_names', 'timestamp_included', 'creation_date']\n",
            "Scaler loaded successfully\n",
            "Train data: 1080 samples from train_data.csv\n",
            "Val data: 231 samples from val_data.csv\n",
            "Test data: 232 samples from test_data.csv\n",
            "train - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "train - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "train - Shape will be: (1080, 10, 54)\n",
            "validation - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "validation - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "validation - Shape will be: (231, 10, 54)\n",
            "test - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "test - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "test - Shape will be: (232, 10, 54)\n",
            "\n",
            "Data validation:\n",
            "Train: X(1080, 10, 54), y(1080,)\n",
            "Val:   X(231, 10, 54), y(231,)\n",
            "Test:  X(232, 10, 54), y(232,)\n",
            "\n",
            "2. Configuration...\n",
            "Data analysis:\n",
            "  Samples: 1080\n",
            "  Features per timestep: 54\n",
            "  Timesteps: 10\n",
            "  Sample/Feature ratio: 2.00\n",
            "Using 'high' risk configuration\n",
            "\n",
            "Final configuration:\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  n_layers: 2\n",
            "  d_ff: 128\n",
            "  dropout: 0.5\n",
            "  learning_rate: 0.0001\n",
            "  weight_decay: 0.001\n",
            "  batch_size: 8\n",
            "  num_epochs: 30\n",
            "  patience: 5\n",
            "\n",
            "3. Creating data loaders...\n",
            "  Train batches: 135\n",
            "  Val batches: 29\n",
            "  Test batches: 29\n",
            "\n",
            "4. Creating model...\n",
            "  Total parameters: 27,949\n",
            "\n",
            "5. Training...\n",
            "Starting training...\n",
            "Epoch   1/30 | Train Loss: 1.386478 | Train R: -0.0171 | Val Loss: 0.054509 | Val R: 0.7900\n",
            "Epoch   2/30 | Train Loss: 1.074754 | Train R: 0.2116 | Val Loss: 0.124460 | Val R: 0.5216\n",
            "Epoch   3/30 | Train Loss: 0.927335 | Train R: 0.3197 | Val Loss: 0.162696 | Val R: 0.3767\n",
            "Epoch   4/30 | Train Loss: 0.864213 | Train R: 0.3660 | Val Loss: 0.180911 | Val R: 0.3069\n",
            "Epoch   5/30 | Train Loss: 0.927757 | Train R: 0.3194 | Val Loss: 0.106650 | Val R: 0.5909\n",
            "Epoch   6/30 | Train Loss: 0.840351 | Train R: 0.3835 | Val Loss: 0.192893 | Val R: 0.2594\n",
            "Early stopping at epoch 6\n",
            "Training completed in 0.18 minutes\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION - CORE METRICS\n",
            "============================================================\n",
            "Train | MSE: 0.424705 | RMSE: 0.651694 | MAE: 0.411695 | R: 0.6885 | Accuracy: 35.00%\n",
            "Val   | MSE: 0.054740 | RMSE: 0.233966 | MAE: 0.135763 | R: 0.7900 | Accuracy: 61.47%\n",
            "Test  | MSE: 0.013419 | RMSE: 0.115840 | MAE: 0.077135 | R: -4.3203 | Accuracy: 71.55%\n",
            "\n",
            "==================================================================\n",
            "FINAL METRICS SUMMARY\n",
            "==================================================================\n",
            "Dataset    |      MSE |     RMSE |      MAE |       R |  Accuracy\n",
            "------------------------------------------------------------------\n",
            "Train      |  0.42471 |   0.6517 |   0.4117 |   0.6885 |    35.00%\n",
            "Val        |  0.05474 |   0.2340 |   0.1358 |   0.7900 |    61.47%\n",
            "Test       |  0.01342 |   0.1158 |   0.0771 |  -4.3203 |    71.55%\n",
            "==================================================================\n",
            "\n",
            "6. Saving model...\n",
            "Model saved to: /kaggle/working/dataset_setelah_modeling_no_dropout/TPIA/simple_lob_transformer.pth\n",
            "\n",
            "7. Creating visualizations...\n",
            "[] Saved loss plot to: /kaggle/working/dataset_setelah_modeling_no_dropout/TPIA/loss_plot.png\n",
            "[] Saved R plot to: /kaggle/working/dataset_setelah_modeling_no_dropout/TPIA/r2_scores.png\n",
            "[] Saved prediction scatter plot to: /kaggle/working/dataset_setelah_modeling_no_dropout/TPIA/predictions_scatter.png\n",
            "\n",
            "================================================================================\n",
            "FINAL PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Test Set Performance:\n",
            "  MSE:  0.013419\n",
            "  RMSE: 0.115840\n",
            "  MAE:  0.077135\n",
            "  R:   -4.3203\n",
            "Accuracy: 71.55%\n",
            "\n",
            "Generalization Analysis:\n",
            "  Train R: 0.6885\n",
            "  Val R:   0.7900\n",
            "  Test R:  -4.3203\n",
            "    Underfitting detected (gap: -0.1015)\n",
            "\n",
            "==================================================\n",
            "CORE METRICS SUMMARY\n",
            "==================================================\n",
            "MSE:  0.013419\n",
            "RMSE: 0.115840\n",
            "MAE:  0.077135\n",
            "R:   -4.3203\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modeling Menggunakan Dropout**"
      ],
      "metadata": {
        "id": "hIAV9yIbPayC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmfhMHCLJDck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554121df-edf0-4c2f-e1fd-51b0039ec2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "LOB Transformer with Comprehensive Dropout & Visualization Loaded!\n",
            "================================================================================\n",
            "ENHANCED LOB TRANSFORMER - COMPREHENSIVE DROPOUT FOR OVERFITTING PREVENTION\n",
            "================================================================================\n",
            "\n",
            "1. Loading data...\n",
            "Loading preprocessed data from: /content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_BRPT\n",
            "Metadata loaded: ['original_data_shape', 'preprocessed_data_shape', 'window_size', 'n_features', 'n_samples_train', 'n_samples_val', 'n_samples_test', 'scaling_method', 'feature_names', 'timestamp_included', 'creation_date']\n",
            "Scaler loaded successfully\n",
            "Train data: 745 samples from train_data.csv\n",
            "Val data: 159 samples from val_data.csv\n",
            "Test data: 161 samples from test_data.csv\n",
            "train - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "train - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "train - Shape will be: (745, 10, 54)\n",
            "validation - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "validation - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "validation - Shape will be: (159, 10, 54)\n",
            "test - Timesteps: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "test - Features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "test - Shape will be: (161, 10, 54)\n",
            "\n",
            "Data validation:\n",
            "Train: X(745, 10, 54), y(745,)\n",
            "Val:   X(159, 10, 54), y(159,)\n",
            "Test:  X(161, 10, 54), y(161,)\n",
            "\n",
            "2. Enhanced Configuration...\n",
            "Data analysis:\n",
            "  Samples: 745\n",
            "  Features per timestep: 54\n",
            "  Timesteps: 10\n",
            "  Sample/Feature ratio: 1.38\n",
            "Using 'high' risk configuration\n",
            "Dropout settings:\n",
            "  General dropout: 0.3\n",
            "  Input dropout: 0.2\n",
            "  Layer dropout: 0.25\n",
            "  Output dropout: 0.3\n",
            "\n",
            "Final enhanced configuration:\n",
            "  d_model: 32\n",
            "  n_heads: 2\n",
            "  n_layers: 2\n",
            "  d_ff: 128\n",
            "  dropout: 0.3\n",
            "  input_dropout: 0.2\n",
            "  layer_dropout: 0.25\n",
            "  output_dropout: 0.3\n",
            "  learning_rate: 0.0005\n",
            "  weight_decay: 0.001\n",
            "  l1_lambda: 1e-05\n",
            "  batch_size: 8\n",
            "  num_epochs: 30\n",
            "  patience: 5\n",
            "\n",
            "3. Creating data loaders...\n",
            "  Train batches: 94\n",
            "  Val batches: 20\n",
            "  Test batches: 21\n",
            "\n",
            "4. Creating enhanced model with comprehensive dropout...\n",
            "  Total parameters: 28,037\n",
            "\n",
            "5. Training enhanced model...\n",
            "Starting enhanced training with comprehensive dropout...\n",
            "Epoch   1/30 | Train Loss: 0.992331 | Train R: 0.0148 | Val Loss: 0.406140 | Val R: 0.5760 | Gap: -0.5612\n",
            "Epoch   2/30 | Train Loss: 0.704911 | Train R: 0.3135 | Val Loss: 0.281650 | Val R: 0.7060 | Gap: -0.3925\n",
            "Epoch   3/30 | Train Loss: 0.537192 | Train R: 0.4780 | Val Loss: 0.241177 | Val R: 0.7483 | Gap: -0.2703\n",
            "Epoch   4/30 | Train Loss: 0.474761 | Train R: 0.5424 | Val Loss: 0.194680 | Val R: 0.7969 | Gap: -0.2545\n",
            "Epoch   5/30 | Train Loss: 0.405442 | Train R: 0.6139 | Val Loss: 0.163323 | Val R: 0.8296 | Gap: -0.2157\n",
            "Epoch   6/30 | Train Loss: 0.356593 | Train R: 0.6644 | Val Loss: 0.139853 | Val R: 0.8541 | Gap: -0.1897\n",
            "Epoch   7/30 | Train Loss: 0.364311 | Train R: 0.6568 | Val Loss: 0.151602 | Val R: 0.8419 | Gap: -0.1851\n",
            "Epoch   8/30 | Train Loss: 0.298221 | Train R: 0.7245 | Val Loss: 0.162410 | Val R: 0.8306 | Gap: -0.1061\n",
            "Epoch   9/30 | Train Loss: 0.342966 | Train R: 0.6783 | Val Loss: 0.104670 | Val R: 0.8908 | Gap: -0.2125\n",
            "Epoch  10/30 | Train Loss: 0.366848 | Train R: 0.6543 | Val Loss: 0.118797 | Val R: 0.8762 | Gap: -0.2220\n",
            "Epoch  11/30 | Train Loss: 0.298861 | Train R: 0.7238 | Val Loss: 0.154191 | Val R: 0.8392 | Gap: -0.1154\n",
            "Epoch  12/30 | Train Loss: 0.288077 | Train R: 0.7352 | Val Loss: 0.133408 | Val R: 0.8609 | Gap: -0.1257\n",
            "Epoch  13/30 | Train Loss: 0.311067 | Train R: 0.7137 | Val Loss: 0.086074 | Val R: 0.9103 | Gap: -0.1966\n",
            "Epoch  14/30 | Train Loss: 0.319637 | Train R: 0.7022 | Val Loss: 0.131039 | Val R: 0.8634 | Gap: -0.1612\n",
            "Epoch  15/30 | Train Loss: 0.285865 | Train R: 0.7371 | Val Loss: 0.091147 | Val R: 0.9050 | Gap: -0.1679\n",
            "Epoch  16/30 | Train Loss: 0.262260 | Train R: 0.7625 | Val Loss: 0.093973 | Val R: 0.9021 | Gap: -0.1396\n",
            "Epoch  17/30 | Train Loss: 0.251710 | Train R: 0.7720 | Val Loss: 0.086297 | Val R: 0.9101 | Gap: -0.1381\n",
            "Epoch  18/30 | Train Loss: 0.260682 | Train R: 0.7630 | Val Loss: 0.083856 | Val R: 0.9126 | Gap: -0.1496\n",
            "Epoch  19/30 | Train Loss: 0.261540 | Train R: 0.7616 | Val Loss: 0.067396 | Val R: 0.9298 | Gap: -0.1682\n",
            "Epoch  20/30 | Train Loss: 0.289517 | Train R: 0.7328 | Val Loss: 0.109624 | Val R: 0.8857 | Gap: -0.1529\n",
            "Epoch  21/30 | Train Loss: 0.259238 | Train R: 0.7643 | Val Loss: 0.079444 | Val R: 0.9172 | Gap: -0.1528\n",
            "Epoch  22/30 | Train Loss: 0.228524 | Train R: 0.7954 | Val Loss: 0.055996 | Val R: 0.9416 | Gap: -0.1462\n",
            "Epoch  23/30 | Train Loss: 0.233137 | Train R: 0.7911 | Val Loss: 0.056339 | Val R: 0.9413 | Gap: -0.1501\n",
            "Epoch  24/30 | Train Loss: 0.241777 | Train R: 0.7815 | Val Loss: 0.074191 | Val R: 0.9226 | Gap: -0.1411\n",
            "Epoch  25/30 | Train Loss: 0.210956 | Train R: 0.8142 | Val Loss: 0.072138 | Val R: 0.9247 | Gap: -0.1105\n",
            "Epoch  26/30 | Train Loss: 0.233155 | Train R: 0.7901 | Val Loss: 0.069885 | Val R: 0.9271 | Gap: -0.1369\n",
            "Epoch  27/30 | Train Loss: 0.230731 | Train R: 0.7957 | Val Loss: 0.069331 | Val R: 0.9276 | Gap: -0.1319\n",
            "Early stopping at epoch 27\n",
            "Training completed in 0.74 minutes\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION - ENHANCED MODEL WITH DROPOUT\n",
            "============================================================\n",
            "Train | MSE: 0.072353 | RMSE: 0.268985 | MAE: 0.180948 | R: 0.9260\n",
            "Val   | MSE: 0.056263 | RMSE: 0.237197 | MAE: 0.160884 | R: 0.9416\n",
            "Test  | MSE: 0.014326 | RMSE: 0.119691 | MAE: 0.119671 | R: -3193.2705\n",
            "\n",
            "Overfitting Analysis:\n",
            "Train-Val Gap: -0.0156\n",
            " Good generalization\n",
            "\n",
            " CREATING CORE METRICS VISUALIZATION...\n",
            " Core metrics visualization failed: name 'create_core_metrics_visualization' is not defined\n",
            "\n",
            "6. Saving enhanced model...\n",
            "Enhanced model saved to: /content/drive/MyDrive/SKRIPSI/dataset_setelah_modeling/minMaxScaller/BRPT/enhanced_lob_transformer.pth\n",
            "\n",
            "================================================================================\n",
            "ENHANCED MODEL PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Test Set Performance:\n",
            "  MSE:  0.014326\n",
            "  RMSE: 0.119691\n",
            "  MAE:  0.119671\n",
            "  R:   -3193.2705\n",
            "  Accuracy (1% error): 100.00%\n",
            "\n",
            "Enhanced Generalization Analysis:\n",
            "  Train R: 0.9260\n",
            "  Val R:   0.9416\n",
            "  Test R:  -3193.2705\n",
            "\n",
            "Dropout Effectiveness:\n",
            "  Train-Val Gap: -0.0156\n",
            "  Val-Test Gap:  3194.2121\n",
            "   Excellent regularization (dropout working well)\n",
            "\n",
            " GENERATING COMPREHENSIVE VISUALIZATIONS...\n",
            "\n",
            " CREATING ALL VISUALIZATIONS...\n",
            "\n",
            "================================================================================\n",
            "DETAILED RESULTS SUMMARY - ENHANCED LOB TRANSFORMER\n",
            "================================================================================\n",
            "\n",
            " MODEL CONFIGURATION:\n",
            "   Architecture: Enhanced LOB Transformer\n",
            "   Model Dimension: 32\n",
            "   Attention Heads: 2\n",
            "   Encoder Layers: 2\n",
            "   Feed Forward Dim: 128\n",
            "\n",
            "  DROPOUT CONFIGURATION:\n",
            "   Input Dropout: 0.200\n",
            "   General Dropout: 0.300\n",
            "   Layer Dropout: 0.250\n",
            "   Output Dropout: 0.300\n",
            "\n",
            " PERFORMANCE METRICS:\n",
            "   Dataset      MSE          RMSE         MAE          R       Accuracy  \n",
            "   ------------------------------------------------------------------------\n",
            "   Train        0.072353     0.268985     0.180948     0.9260   46.98%\n",
            "   Val          0.056263     0.237197     0.160884     0.9416   61.01%\n",
            "   Test         0.014326     0.119691     0.119671     -3193.2705 100.00%\n",
            "\n",
            " OVERFITTING ANALYSIS:\n",
            "   Train R: 0.9260\n",
            "   Val R:   0.9416\n",
            "   Test R:  -3193.2705\n",
            "   Train-Val Gap: -0.0156\n",
            "   Val-Test Gap:  3194.2121\n",
            "\n",
            " INTERPRETATION:\n",
            "    EXCELLENT REGULARIZATION\n",
            "      Dropout configuration is working well\n",
            "     Significant val-test gap detected\n",
            "\n",
            " MODEL QUALITY ASSESSMENT:\n",
            "    POOR performance (R < 0.4)\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "CREATING COMPREHENSIVE VISUALIZATIONS\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import time\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Additional imports for visualization\n",
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    import plotly.express as px\n",
        "    PLOTLY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PLOTLY_AVAILABLE = False\n",
        "    print(\"Plotly not available. Interactive visualizations will be skipped.\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('default')\n",
        "try:\n",
        "    sns.set_palette(\"husl\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ================================\n",
        "# 1. SIMPLIFIED DATA LOADER\n",
        "# ================================\n",
        "\n",
        "def load_preprocessed_csv_data(data_dir, debug=True):\n",
        "    \"\"\"\n",
        "    Simplified data loader focusing on core functionality\n",
        "    \"\"\"\n",
        "    print(f\"Loading preprocessed data from: {data_dir}\")\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_path = os.path.join(data_dir, 'metadata.json')\n",
        "    if not os.path.exists(metadata_path):\n",
        "        raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n",
        "\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(f\"Metadata loaded: {list(metadata.keys())}\")\n",
        "\n",
        "    # Load scaler\n",
        "    scaler_path = os.path.join(data_dir, 'scaler.pkl')\n",
        "    scaler = None\n",
        "    if os.path.exists(scaler_path):\n",
        "        with open(scaler_path, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        print(\"Scaler loaded successfully\")\n",
        "\n",
        "    # Load data files with flexible naming\n",
        "    datasets = {}\n",
        "    file_mappings = {\n",
        "        'train': ['train_data.csv'],\n",
        "        'val': ['val_data.csv', 'validation_data.csv'],\n",
        "        'test': ['test_data.csv']\n",
        "    }\n",
        "\n",
        "    for split, possible_names in file_mappings.items():\n",
        "        found = False\n",
        "        for file_name in possible_names:\n",
        "            file_path = os.path.join(data_dir, file_name)\n",
        "            if os.path.exists(file_path):\n",
        "                datasets[split] = pd.read_csv(file_path)\n",
        "                print(f\"{split.capitalize()} data: {len(datasets[split])} samples from {file_name}\")\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            raise FileNotFoundError(f\"{split.capitalize()} data not found. Tried: {possible_names}\")\n",
        "\n",
        "    # Extract features and targets\n",
        "    def extract_features_targets(df, debug_name=\"\"):\n",
        "        # Find target column\n",
        "        target_col = None\n",
        "        if 'target' in df.columns:\n",
        "            target_col = 'target'\n",
        "        else:\n",
        "            target_candidates = [col for col in df.columns if any(x in col.lower() for x in ['mid_price', 'target', 'price'])]\n",
        "            if target_candidates:\n",
        "                target_col = target_candidates[0]\n",
        "                print(f\"Using '{target_col}' as target for {debug_name}\")\n",
        "\n",
        "        if target_col is None:\n",
        "            raise ValueError(f\"No target column found in {debug_name} data\")\n",
        "\n",
        "        y = df[target_col].values\n",
        "\n",
        "        # Extract features (look for time-feature pattern)\n",
        "        feature_pattern = re.compile(r't(\\d+)_f(\\d+)')\n",
        "        feature_cols = []\n",
        "        timesteps = set()\n",
        "        features = set()\n",
        "\n",
        "        for col in df.columns:\n",
        "            match = feature_pattern.search(col)\n",
        "            if match:\n",
        "                t, f = int(match.group(1)), int(match.group(2))\n",
        "                timesteps.add(t)\n",
        "                features.add(f)\n",
        "                feature_cols.append((col, t, f))\n",
        "\n",
        "        if not feature_cols:\n",
        "            # Fallback: use all non-target columns\n",
        "            exclude_cols = {'target', 'timestamp', 'index', target_col}\n",
        "            feature_cols = [(col, 0, i) for i, col in enumerate(df.columns)\n",
        "                           if col not in exclude_cols and not any(x in col.lower() for x in ['timestamp', 'index'])]\n",
        "            print(f\"Warning: No time-feature pattern found, using {len(feature_cols)} columns as features\")\n",
        "            timesteps = {0}\n",
        "            features = set(range(len(feature_cols)))\n",
        "\n",
        "        # Determine dimensions\n",
        "        n_timesteps = len(timesteps)\n",
        "        n_features = len(features)\n",
        "        n_samples = len(df)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"{debug_name} - Timesteps: {sorted(timesteps)}\")\n",
        "            print(f\"{debug_name} - Features: {sorted(features)}\")\n",
        "            print(f\"{debug_name} - Shape will be: ({n_samples}, {n_timesteps}, {n_features})\")\n",
        "\n",
        "        # Create 3D array\n",
        "        X = np.zeros((n_samples, n_timesteps, n_features))\n",
        "\n",
        "        # Fill array\n",
        "        for col_name, t, f in feature_cols:\n",
        "            if t in timesteps and f in features:\n",
        "                t_idx = sorted(timesteps).index(t)\n",
        "                f_idx = sorted(features).index(f)\n",
        "\n",
        "                # Ensure column exists and has valid data\n",
        "                if col_name in df.columns:\n",
        "                    col_data = df[col_name].values\n",
        "                    # Replace inf values with finite numbers\n",
        "                    col_data = np.where(np.isinf(col_data), 0, col_data)\n",
        "                    X[:, t_idx, f_idx] = col_data\n",
        "\n",
        "        # Handle NaN and inf values\n",
        "        X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        # Clip extreme values to prevent gradient issues\n",
        "        X = np.clip(X, -1e6, 1e6)\n",
        "        y = np.clip(y, -1e6, 1e6)\n",
        "\n",
        "        # Clean NaN/inf in targets\n",
        "        if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n",
        "            print(f\"Warning: Cleaning NaN/inf in {debug_name} targets\")\n",
        "            y = np.nan_to_num(y, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        return X, y, n_timesteps, n_features\n",
        "\n",
        "    # Process all datasets\n",
        "    X_train, y_train, n_timesteps, n_features = extract_features_targets(datasets['train'], \"train\")\n",
        "    X_val, y_val, _, _ = extract_features_targets(datasets['val'], \"validation\")\n",
        "    X_test, y_test, _, _ = extract_features_targets(datasets['test'], \"test\")\n",
        "\n",
        "    # Data validation\n",
        "    print(f\"\\nData validation:\")\n",
        "    print(f\"Train: X{X_train.shape}, y{y_train.shape}\")\n",
        "    print(f\"Val:   X{X_val.shape}, y{y_val.shape}\")\n",
        "    print(f\"Test:  X{X_test.shape}, y{y_test.shape}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    data = {\n",
        "        'X_train': torch.FloatTensor(X_train),\n",
        "        'y_train': torch.FloatTensor(y_train),\n",
        "        'X_val': torch.FloatTensor(X_val),\n",
        "        'y_val': torch.FloatTensor(y_val),\n",
        "        'X_test': torch.FloatTensor(X_test),\n",
        "        'y_test': torch.FloatTensor(y_test),\n",
        "        'scaler': scaler,\n",
        "        'n_timesteps': n_timesteps,\n",
        "        'n_features': n_features\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "# ================================\n",
        "# 2. ENHANCED TRANSFORMER MODEL WITH COMPREHENSIVE DROPOUT\n",
        "# ================================\n",
        "\n",
        "class EnhancedPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced positional encoding with dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=1000, dropout=0.1):\n",
        "        super(EnhancedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embedding_dropout = nn.Dropout(p=dropout * 0.5)  # Additional embedding dropout\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        # Apply embedding dropout before adding positional encoding\n",
        "        x = self.embedding_dropout(x)\n",
        "        # Add positional encoding and apply final dropout\n",
        "        return self.dropout(x + self.pe[:, :seq_len, :])\n",
        "\n",
        "class EnhancedLOBTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced LOB Transformer with comprehensive dropout for overfitting prevention\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, n_heads=8, n_layers=4, d_ff=512,\n",
        "                 dropout=0.1, input_dropout=0.1, layer_dropout=0.1, output_dropout=0.2,\n",
        "                 max_seq_len=100):\n",
        "        super(EnhancedLOBTransformer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input processing with dropout\n",
        "        self.input_dropout = nn.Dropout(p=input_dropout)\n",
        "        self.input_norm = nn.LayerNorm(input_dim)\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.projection_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Enhanced positional encoding with dropout\n",
        "        self.pos_encoding = EnhancedPositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Transformer encoder with enhanced dropout\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Additional dropout between transformer layers\n",
        "        self.inter_layer_dropout = nn.Dropout(p=layer_dropout)\n",
        "\n",
        "        # Enhanced output layers with comprehensive dropout\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.pool_dropout = nn.Dropout(p=output_dropout * 0.5)\n",
        "\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.LayerNorm(d_model // 2),  # Layer normalization for stability\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(output_dropout),\n",
        "\n",
        "            nn.Linear(d_model // 2, d_model // 4),\n",
        "            nn.LayerNorm(d_model // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(output_dropout * 0.8),\n",
        "\n",
        "            nn.Linear(d_model // 4, d_model // 8),\n",
        "            nn.LayerNorm(d_model // 8),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(output_dropout * 0.6),\n",
        "\n",
        "            nn.Linear(d_model // 8, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input processing with dropout\n",
        "        x = self.input_dropout(x)  # Input-level dropout\n",
        "        x = self.input_norm(x)\n",
        "        x = self.input_projection(x)\n",
        "        x = self.projection_dropout(x)  # Post-projection dropout\n",
        "\n",
        "        # Add positional encoding (includes its own dropout)\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Transformer encoding with inter-layer dropout\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "        x = self.inter_layer_dropout(x)  # Additional dropout after transformer\n",
        "\n",
        "        # Global average pooling with dropout\n",
        "        x = x.transpose(1, 2)  # [batch, features, seq_len]\n",
        "        x = self.global_pool(x).squeeze(-1)  # [batch, features]\n",
        "        x = self.pool_dropout(x)  # Post-pooling dropout\n",
        "\n",
        "        # Output prediction (includes multiple dropout layers)\n",
        "        output = self.output_head(x).squeeze(-1)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ================================\n",
        "# 3. ENHANCED TRAINER WITH REGULARIZATION\n",
        "# ================================\n",
        "\n",
        "class EnhancedTrainer:\n",
        "    \"\"\"\n",
        "    Enhanced training system with additional regularization techniques\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_loader, val_loader, test_loader, config):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.config = config\n",
        "\n",
        "        # Loss function with label smoothing option\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # L1 regularization coefficient\n",
        "        self.l1_lambda = config.get('l1_lambda', 0.0)\n",
        "\n",
        "        # Optimizer with gradient clipping\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config['learning_rate'],\n",
        "            weight_decay=config['weight_decay'],\n",
        "            eps=1e-8,\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "\n",
        "        # Enhanced learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True,\n",
        "            min_lr=1e-7\n",
        "        )\n",
        "\n",
        "        # Tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def calculate_l1_loss(self):\n",
        "        \"\"\"Calculate L1 regularization loss\"\"\"\n",
        "        l1_loss = 0\n",
        "        for param in self.model.parameters():\n",
        "            l1_loss += torch.norm(param, 1)\n",
        "        return l1_loss\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        for batch_idx, (X, y) in enumerate(self.train_loader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Skip problematic batches\n",
        "            if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                continue\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(X)\n",
        "\n",
        "            # Skip if output contains NaN\n",
        "            if torch.isnan(outputs).any():\n",
        "                continue\n",
        "\n",
        "            # Calculate primary loss\n",
        "            loss = self.criterion(outputs, y)\n",
        "\n",
        "            # Add L1 regularization if specified\n",
        "            if self.l1_lambda > 0:\n",
        "                l1_loss = self.calculate_l1_loss()\n",
        "                loss = loss + self.l1_lambda * l1_loss\n",
        "\n",
        "            # Skip if loss is NaN\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "\n",
        "            # Backward pass with gradient clipping\n",
        "            try:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                all_preds.extend(outputs.detach().cpu().numpy())\n",
        "                all_targets.extend(y.detach().cpu().numpy())\n",
        "                valid_batches += 1\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Skipping batch {batch_idx} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in self.val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(X)\n",
        "\n",
        "                if torch.isnan(outputs).any():\n",
        "                    continue\n",
        "\n",
        "                loss = self.criterion(outputs, y)\n",
        "\n",
        "                if torch.isnan(loss):\n",
        "                    continue\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(y.cpu().numpy())\n",
        "                valid_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Starting enhanced training with comprehensive dropout...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config['num_epochs']):\n",
        "            # Training\n",
        "            train_loss, train_preds, train_targets = self.train_epoch()\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_preds, val_targets = self.validate()\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Calculate R for monitoring\n",
        "            train_r2 = r2_score(train_targets, train_preds) if len(train_targets) > 0 else 0\n",
        "            val_r2 = r2_score(val_targets, val_preds) if len(val_targets) > 0 else 0\n",
        "\n",
        "            # Calculate overfitting indicator\n",
        "            overfitting_gap = train_r2 - val_r2\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Print progress with overfitting monitoring\n",
        "            print(f\"Epoch {epoch+1:3d}/{self.config['num_epochs']} | \"\n",
        "                  f\"Train Loss: {train_loss:.6f} | Train R: {train_r2:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.6f} | Val R: {val_r2:.4f} | \"\n",
        "                  f\"Gap: {overfitting_gap:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.config['patience']:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        if self.best_model_state is not None:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"Training completed in {training_time/60:.2f} minutes\")\n",
        "\n",
        "        return self.final_evaluation()\n",
        "\n",
        "    def evaluate_loader(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        valid_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                if torch.isnan(X).any() or torch.isnan(y).any():\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(X)\n",
        "\n",
        "                if torch.isnan(outputs).any():\n",
        "                    continue\n",
        "\n",
        "                loss = self.criterion(outputs, y)\n",
        "\n",
        "                if not torch.isnan(loss):\n",
        "                    total_loss += loss.item()\n",
        "                    valid_batches += 1\n",
        "\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / max(1, valid_batches)\n",
        "        return avg_loss, all_preds, all_targets\n",
        "\n",
        "    def calculate_core_metrics(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate only core metrics: MSE, RMSE, MAE, R\n",
        "        \"\"\"\n",
        "        if len(y_true) == 0 or len(y_pred) == 0:\n",
        "            return {'MSE': float('nan'), 'RMSE': float('nan'), 'MAE': float('nan'), 'R2': float('nan')}\n",
        "\n",
        "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        return {\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'R2': r2\n",
        "        }\n",
        "\n",
        "    def final_evaluation(self):\n",
        "        \"\"\"\n",
        "        Final evaluation with core metrics only\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FINAL EVALUATION - ENHANCED MODEL WITH DROPOUT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = {}\n",
        "        for name, loader in [('train', self.train_loader), ('val', self.val_loader), ('test', self.test_loader)]:\n",
        "            loss, preds, targets = self.evaluate_loader(loader)\n",
        "            metrics = self.calculate_core_metrics(targets, preds)\n",
        "\n",
        "            results[name] = {\n",
        "                'loss': loss,\n",
        "                'predictions': preds,\n",
        "                'targets': targets,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "\n",
        "            print(f\"{name.capitalize():5} | MSE: {metrics['MSE']:.6f} | RMSE: {metrics['RMSE']:.6f} | \"\n",
        "                  f\"MAE: {metrics['MAE']:.6f} | R: {metrics['R2']:.4f}\")\n",
        "\n",
        "        # Overfitting analysis\n",
        "        train_r2 = results['train']['metrics']['R2']\n",
        "        val_r2 = results['val']['metrics']['R2']\n",
        "        test_r2 = results['test']['metrics']['R2']\n",
        "        overfitting_gap = train_r2 - val_r2\n",
        "\n",
        "        print(f\"\\nOverfitting Analysis:\")\n",
        "        print(f\"Train-Val Gap: {overfitting_gap:.4f}\")\n",
        "        if overfitting_gap > 0.1:\n",
        "            print(\"  High overfitting detected\")\n",
        "        elif overfitting_gap > 0.05:\n",
        "            print(\"  Moderate overfitting detected\")\n",
        "        else:\n",
        "            print(\" Good generalization\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# ================================\n",
        "# 4. ENHANCED CONFIGURATION\n",
        "# ================================\n",
        "\n",
        "def get_enhanced_config(data_info, risk_level='medium'):\n",
        "    \"\"\"\n",
        "    Get enhanced configuration with comprehensive dropout settings\n",
        "    \"\"\"\n",
        "    n_samples = data_info.get('n_train_samples', 1000)\n",
        "    n_features = data_info.get('n_features', 10)\n",
        "    n_timesteps = data_info.get('n_timesteps', 10)\n",
        "\n",
        "    total_features = n_features * n_timesteps\n",
        "    sample_feature_ratio = n_samples / total_features\n",
        "\n",
        "    print(f\"Data analysis:\")\n",
        "    print(f\"  Samples: {n_samples}\")\n",
        "    print(f\"  Features per timestep: {n_features}\")\n",
        "    print(f\"  Timesteps: {n_timesteps}\")\n",
        "    print(f\"  Sample/Feature ratio: {sample_feature_ratio:.2f}\")\n",
        "\n",
        "    # Enhanced configurations with comprehensive dropout\n",
        "    configs = {\n",
        "        'low': {\n",
        "            'd_model': 256,\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 6,\n",
        "            'd_ff': 1024,\n",
        "            'dropout': 0.3,\n",
        "            'input_dropout': 0.2,\n",
        "            'layer_dropout': 0.3,\n",
        "            'output_dropout': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'weight_decay': 1e-5,\n",
        "            'l1_lambda': 0.0,\n",
        "            'batch_size': 32,\n",
        "            'num_epochs': 15,\n",
        "            'patience': 5\n",
        "        },\n",
        "        'medium': {\n",
        "            'd_model': 128,\n",
        "            'n_heads': 8,\n",
        "            'n_layers': 4,\n",
        "            'd_ff': 512,\n",
        "            'dropout': 0.4,\n",
        "            'input_dropout': 0.25,\n",
        "            'layer_dropout': 0.35,\n",
        "            'output_dropout': 0.4,\n",
        "            'learning_rate': 0.0005,\n",
        "            'weight_decay': 1e-4,\n",
        "            'l1_lambda': 1e-6,\n",
        "            'batch_size': 16,\n",
        "            'num_epochs': 15,\n",
        "            'patience': 5\n",
        "        },\n",
        "        'high': {\n",
        "            'd_model': 32,\n",
        "            'n_heads': 2,\n",
        "            'n_layers': 2,\n",
        "            'd_ff': 128,\n",
        "            'dropout': 0.3,\n",
        "            'input_dropout': 0.2,\n",
        "            'layer_dropout': 0.25,\n",
        "            'output_dropout': 0.3,\n",
        "            'learning_rate': 0.0005,\n",
        "            'weight_decay': 1e-3,\n",
        "            'l1_lambda': 1e-5,\n",
        "            'batch_size': 8,\n",
        "            'num_epochs': 30,\n",
        "            'patience': 5\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Auto-detect risk level based on sample/feature ratio\n",
        "    if sample_feature_ratio < 5:\n",
        "        auto_risk = 'high'\n",
        "    elif sample_feature_ratio < 15:\n",
        "        auto_risk = 'medium'\n",
        "    else:\n",
        "        auto_risk = 'low'\n",
        "\n",
        "    if risk_level == 'auto':\n",
        "        risk_level = auto_risk\n",
        "\n",
        "    config = configs[risk_level].copy()\n",
        "    config['batch_size'] = min(config['batch_size'], max(4, n_samples // 10))\n",
        "\n",
        "    print(f\"Using '{risk_level}' risk configuration\")\n",
        "    print(f\"Dropout settings:\")\n",
        "    print(f\"  General dropout: {config['dropout']}\")\n",
        "    print(f\"  Input dropout: {config['input_dropout']}\")\n",
        "    print(f\"  Layer dropout: {config['layer_dropout']}\")\n",
        "    print(f\"  Output dropout: {config['output_dropout']}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# ================================\n",
        "# 5. ENHANCED PREDICTION INTERFACE\n",
        "# ================================\n",
        "\n",
        "class EnhancedLOBPredictor:\n",
        "    \"\"\"\n",
        "    Enhanced prediction interface for the LOB model with dropout control\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, scaler_path=None):\n",
        "        # Load model\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "        # Get model architecture\n",
        "        arch = checkpoint['model_architecture']\n",
        "        self.model = EnhancedLOBTransformer(\n",
        "            input_dim=arch['input_dim'],\n",
        "            d_model=arch['d_model'],\n",
        "            n_heads=arch['n_heads'],\n",
        "            n_layers=arch['n_layers'],\n",
        "            d_ff=arch['d_ff'],\n",
        "            dropout=arch['dropout'],\n",
        "            input_dropout=arch['input_dropout'],\n",
        "            layer_dropout=arch['layer_dropout'],\n",
        "            output_dropout=arch['output_dropout'],\n",
        "            max_seq_len=arch['max_seq_len']\n",
        "        )\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(device)\n",
        "        self.model.eval()  # Set to evaluation mode (disables dropout)\n",
        "\n",
        "        # Load scaler\n",
        "        self.scaler = None\n",
        "        if scaler_path and os.path.exists(scaler_path):\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "\n",
        "        self.config = checkpoint['config']\n",
        "        print(f\"Enhanced model loaded successfully!\")\n",
        "        print(f\"Input shape expected: (batch_size, {arch['max_seq_len']}, {arch['input_dim']})\")\n",
        "        print(f\"Dropout configuration loaded: {arch['dropout']:.2f} (disabled during inference)\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions on new data (dropout disabled automatically in eval mode)\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(X):\n",
        "            X = torch.FloatTensor(X)\n",
        "\n",
        "        if len(X.shape) == 2:\n",
        "            X = X.unsqueeze(0)\n",
        "\n",
        "        X = X.to(device)\n",
        "\n",
        "        self.model.eval()  # Ensure dropout is disabled\n",
        "        with torch.no_grad():\n",
        "            prediction = self.model(X)\n",
        "            return prediction.cpu().numpy().squeeze()\n",
        "\n",
        "    def predict_with_uncertainty(self, X, n_samples=10):\n",
        "        \"\"\"\n",
        "        Predict with uncertainty estimation using Monte Carlo Dropout\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(X):\n",
        "            X = torch.FloatTensor(X)\n",
        "\n",
        "        if len(X.shape) == 2:\n",
        "            X = X.unsqueeze(0)\n",
        "\n",
        "        X = X.to(device)\n",
        "\n",
        "        # Enable dropout for uncertainty estimation\n",
        "        self.model.train()\n",
        "\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(n_samples):\n",
        "                pred = self.model(X)\n",
        "                predictions.append(pred.cpu().numpy().squeeze())\n",
        "\n",
        "        # Disable dropout again\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "        mean_pred = np.mean(predictions)\n",
        "        std_pred = np.std(predictions)\n",
        "\n",
        "        return mean_pred, std_pred\n",
        "\n",
        "# ================================\n",
        "# 6. COMPREHENSIVE VISUALIZATION MODULE\n",
        "# ================================\n",
        "\n",
        "def create_comprehensive_visualization(model, trainer, results, config, save_dir=None):\n",
        "    \"\"\"\n",
        "    Membuat visualisasi komprehensif untuk hasil training LOB Transformer\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig = plt.figure(figsize=(20, 24))\n",
        "\n",
        "    # 1. Training Loss Curves\n",
        "    plt.subplot(4, 3, 1)\n",
        "    epochs = range(1, len(trainer.train_losses) + 1)\n",
        "    plt.plot(epochs, trainer.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    plt.plot(epochs, trainer.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add early stopping point if applicable\n",
        "    if trainer.patience_counter >= config['patience']:\n",
        "        best_epoch = len(trainer.train_losses) - trainer.patience_counter\n",
        "        plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7,\n",
        "                   label=f'Early Stop (Epoch {best_epoch})')\n",
        "        plt.legend()\n",
        "\n",
        "    # 2. Predictions vs Actual - Training Set\n",
        "    plt.subplot(4, 3, 2)\n",
        "    train_preds = results['train']['predictions']\n",
        "    train_targets = results['train']['targets']\n",
        "\n",
        "    plt.scatter(train_targets, train_preds, alpha=0.6, s=20, color='blue')\n",
        "    min_val = min(min(train_targets), min(train_preds))\n",
        "    max_val = max(max(train_targets), max(train_preds))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    test_r2 = results[\"test\"][\"metrics\"][\"R2\"]\n",
        "    r2_text = f\"R = {test_r2:.4f}\" if -10 < test_r2 < 1 else \"R = N/A\"\n",
        "\n",
        "    plt.title(f'Test: Predictions vs Actual\\n{r2_text}', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Predictions vs Actual - Validation Set\n",
        "    plt.subplot(4, 3, 3)\n",
        "    val_preds = results['val']['predictions']\n",
        "    val_targets = results['val']['targets']\n",
        "\n",
        "    plt.scatter(val_targets, val_preds, alpha=0.6, s=20, color='orange')\n",
        "    min_val = min(min(val_targets), min(val_preds))\n",
        "    max_val = max(max(val_targets), max(val_preds))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Validation: Predictions vs Actual\\nR = {results[\"val\"][\"metrics\"][\"R2\"]:.4f}',\n",
        "              fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Predictions vs Actual - Test Set\n",
        "    plt.subplot(4, 3, 4)\n",
        "    test_preds = results['test']['predictions']\n",
        "    test_targets = results['test']['targets']\n",
        "\n",
        "    plt.scatter(test_targets, test_preds, alpha=0.6, s=20, color='green')\n",
        "    min_val = min(min(test_targets), min(test_preds))\n",
        "    max_val = max(max(test_targets), max(test_preds))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Test: Predictions vs Actual\\nR = {results[\"test\"][\"metrics\"][\"R2\"]:.4f}',\n",
        "              fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Residual Analysis - Test Set\n",
        "    plt.subplot(4, 3, 5)\n",
        "    test_residuals = np.array(test_preds) - np.array(test_targets)\n",
        "    plt.scatter(test_preds, test_residuals, alpha=0.6, s=20, color='purple')\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residual Analysis (Test Set)', fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Residual Distribution\n",
        "    plt.subplot(4, 3, 6)\n",
        "    plt.hist(test_residuals, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    mu, sigma = stats.norm.fit(test_residuals)\n",
        "    x = np.linspace(min(test_residuals), max(test_residuals), 100)\n",
        "    plt.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Normal Fit (={mu:.4f}, ={sigma:.4f})')\n",
        "    plt.xlabel('Residuals')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Residual Distribution', fontsize=12, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Performance Metrics Comparison\n",
        "    plt.subplot(4, 3, 7)\n",
        "    datasets = ['Train', 'Validation', 'Test']\n",
        "    r2_scores = [results['train']['metrics']['R2'],\n",
        "                 results['val']['metrics']['R2'],\n",
        "                 results['test']['metrics']['R2']]\n",
        "\n",
        "    bars = plt.bar(datasets, r2_scores, color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "    plt.ylabel('R Score')\n",
        "    plt.title('R Score Comparison', fontsize=12, fontweight='bold')\n",
        "    plt.ylim(0, max(1, max(r2_scores) * 1.1))\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars, r2_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 8. Overfitting Analysis\n",
        "    plt.subplot(4, 3, 8)\n",
        "    train_r2 = results['train']['metrics']['R2']\n",
        "    val_r2 = results['val']['metrics']['R2']\n",
        "    test_r2 = results['test']['metrics']['R2']\n",
        "\n",
        "    overfitting_gap = train_r2 - val_r2\n",
        "    generalization_gap = val_r2 - test_r2\n",
        "\n",
        "    gaps = ['Train-Val Gap', 'Val-Test Gap']\n",
        "    gap_values = [overfitting_gap, generalization_gap]\n",
        "    colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' for gap in gap_values]\n",
        "\n",
        "    bars = plt.bar(gaps, gap_values, color=colors, alpha=0.7)\n",
        "    plt.ylabel('Performance Gap')\n",
        "    plt.title('Overfitting Analysis', fontsize=12, fontweight='bold')\n",
        "    plt.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Moderate Threshold')\n",
        "    plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='High Threshold')\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, gap in zip(bars, gap_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{gap:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 9. Error Metrics Comparison\n",
        "    plt.subplot(4, 3, 9)\n",
        "    metrics = ['MSE', 'RMSE', 'MAE']\n",
        "    test_metrics = results['test']['metrics']\n",
        "    metric_values = [test_metrics['MSE'], test_metrics['RMSE'], test_metrics['MAE']]\n",
        "\n",
        "    bars = plt.bar(metrics, metric_values, color='lightcoral', alpha=0.7)\n",
        "    plt.ylabel('Error Value')\n",
        "    plt.title('Test Set Error Metrics', fontsize=12, fontweight='bold')\n",
        "    plt.yscale('log')  # Log scale for better visualization\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, metric_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n",
        "                f'{value:.6f}', ha='center', va='bottom', fontweight='bold', rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 10. Time Series Prediction Sample (first 100 points)\n",
        "    plt.subplot(4, 3, 10)\n",
        "    n_points = min(100, len(test_targets))\n",
        "    indices = range(n_points)\n",
        "\n",
        "    plt.plot(indices, test_targets[:n_points], 'b-', label='Actual', linewidth=2, alpha=0.8)\n",
        "    plt.plot(indices, test_preds[:n_points], 'r-', label='Predicted', linewidth=2, alpha=0.8)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Time Series Prediction Sample', fontsize=12, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 11. Learning Rate Schedule (placeholder)\n",
        "    plt.subplot(4, 3, 11)\n",
        "    plt.plot(epochs, [config['learning_rate']] * len(epochs), 'g-', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 12. Dropout Configuration Visualization\n",
        "    plt.subplot(4, 3, 12)\n",
        "    dropout_types = ['Input', 'Projection', 'Layer', 'Output']\n",
        "    dropout_values = [\n",
        "        config.get('input_dropout', 0),\n",
        "        config.get('dropout', 0),\n",
        "        config.get('layer_dropout', 0),\n",
        "        config.get('output_dropout', 0)\n",
        "    ]\n",
        "\n",
        "    bars = plt.bar(dropout_types, dropout_values, color='steelblue', alpha=0.7)\n",
        "    plt.ylabel('Dropout Rate')\n",
        "    plt.title('Dropout Configuration', fontsize=12, fontweight='bold')\n",
        "    plt.ylim(0, max(dropout_values) * 1.2)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, dropout_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the comprehensive visualization\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Comprehensive visualization saved to: {save_dir}/comprehensive_analysis.png\")\n",
        "\n",
        "    # plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def create_interactive_visualization(results, save_dir=None):\n",
        "    \"\"\"\n",
        "    Membuat visualisasi interaktif menggunakan Plotly\n",
        "    \"\"\"\n",
        "    if not PLOTLY_AVAILABLE:\n",
        "        print(\"Plotly not available. Skipping interactive visualization.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nCreating interactive visualizations...\")\n",
        "\n",
        "    # Create subplot structure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Training vs Validation Predictions', 'Test Set Predictions',\n",
        "                       'Residual Analysis', 'Performance Metrics'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "    )\n",
        "\n",
        "    # 1. Training vs Validation Predictions\n",
        "    train_targets = results['train']['targets']\n",
        "    train_preds = results['train']['predictions']\n",
        "    val_targets = results['val']['targets']\n",
        "    val_preds = results['val']['predictions']\n",
        "\n",
        "    # Perfect prediction line\n",
        "    min_val = min(min(train_targets + val_targets), min(train_preds + val_preds))\n",
        "    max_val = max(max(train_targets + val_targets), max(train_preds + val_preds))\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                  mode='lines', name='Perfect Prediction',\n",
        "                  line=dict(color='red', dash='dash')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=train_targets, y=train_preds, mode='markers',\n",
        "                  name=f'Train (R={results[\"train\"][\"metrics\"][\"R2\"]:.4f})',\n",
        "                  marker=dict(color='blue', opacity=0.6)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=val_targets, y=val_preds, mode='markers',\n",
        "                  name=f'Validation (R={results[\"val\"][\"metrics\"][\"R2\"]:.4f})',\n",
        "                  marker=dict(color='orange', opacity=0.6)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # 2. Test Set Predictions\n",
        "    test_targets = results['test']['targets']\n",
        "    test_preds = results['test']['predictions']\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[min(test_targets), max(test_targets)],\n",
        "                  y=[min(test_targets), max(test_targets)],\n",
        "                  mode='lines', name='Perfect Prediction',\n",
        "                  line=dict(color='red', dash='dash'), showlegend=False),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=test_targets, y=test_preds, mode='markers',\n",
        "                  name=f'Test (R={results[\"test\"][\"metrics\"][\"R2\"]:.4f})',\n",
        "                  marker=dict(color='green', opacity=0.6)),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # 3. Residual Analysis\n",
        "    test_residuals = np.array(test_preds) - np.array(test_targets)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=test_preds, y=test_residuals, mode='markers',\n",
        "                  name='Residuals', marker=dict(color='purple', opacity=0.6)),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[min(test_preds), max(test_preds)], y=[0, 0],\n",
        "                  mode='lines', name='Zero Line',\n",
        "                  line=dict(color='red', dash='dash'), showlegend=False),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # 4. Performance Metrics\n",
        "    datasets = ['Train', 'Validation', 'Test']\n",
        "    r2_scores = [results['train']['metrics']['R2'],\n",
        "                 results['val']['metrics']['R2'],\n",
        "                 results['test']['metrics']['R2']]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=datasets, y=r2_scores, name='R Scores',\n",
        "               marker=dict(color=['blue', 'orange', 'green'])),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text=\"Enhanced LOB Transformer - Interactive Analysis\",\n",
        "        title_x=0.5,\n",
        "        showlegend=True,\n",
        "        height=800,\n",
        "        font=dict(size=12)\n",
        "    )\n",
        "\n",
        "    # Update axes labels\n",
        "    fig.update_xaxes(title_text=\"Actual Values\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Predicted Values\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Actual Values\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Predicted Values\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Predicted Values\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Residuals\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Dataset\", row=2, col=2)\n",
        "    fig.update_yaxes(title_text=\"R Score\", row=2, col=2)\n",
        "\n",
        "    if save_dir:\n",
        "        fig.write_html(f'{save_dir}/interactive_analysis.html')\n",
        "        print(f\"Interactive visualization saved to: {save_dir}/interactive_analysis.html\")\n",
        "\n",
        "    # fig.show()\n",
        "\n",
        "\n",
        "def create_uncertainty_visualization(model_predictor, X_test, y_test, n_samples=20, save_dir=None):\n",
        "    \"\"\"\n",
        "    Membuat visualisasi uncertainty estimation menggunakan Monte Carlo Dropout\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating uncertainty estimation visualization...\")\n",
        "\n",
        "    # Select subset for uncertainty analysis (computational efficiency)\n",
        "    n_points = min(100, len(X_test))\n",
        "    indices = np.random.choice(len(X_test), n_points, replace=False)\n",
        "    X_subset = X_test[indices]\n",
        "    y_subset = y_test[indices]\n",
        "\n",
        "    predictions = []\n",
        "    uncertainties = []\n",
        "\n",
        "    # Generate predictions with uncertainty\n",
        "    for i in range(len(X_subset)):\n",
        "        mean_pred, std_pred = model_predictor.predict_with_uncertainty(\n",
        "            X_subset[i:i+1], n_samples=n_samples\n",
        "        )\n",
        "        predictions.append(mean_pred)\n",
        "        uncertainties.append(std_pred)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    uncertainties = np.array(uncertainties)\n",
        "\n",
        "    # Create uncertainty visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # 1. Predictions with uncertainty bands\n",
        "    axes[0, 0].scatter(y_subset, predictions, alpha=0.6, color='blue', s=30)\n",
        "    axes[0, 0].errorbar(y_subset, predictions, yerr=uncertainties,\n",
        "                       fmt='none', alpha=0.3, color='red')\n",
        "    min_val = min(min(y_subset), min(predictions))\n",
        "    max_val = max(max(y_subset), max(predictions))\n",
        "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Actual Values')\n",
        "    axes[0, 0].set_ylabel('Predicted Values')\n",
        "    axes[0, 0].set_title('Predictions with Uncertainty Bands')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Uncertainty vs Prediction Error\n",
        "    prediction_errors = np.abs(predictions - y_subset)\n",
        "    axes[0, 1].scatter(uncertainties, prediction_errors, alpha=0.6, color='green', s=30)\n",
        "    axes[0, 1].set_xlabel('Prediction Uncertainty (Std)')\n",
        "    axes[0, 1].set_ylabel('Prediction Error')\n",
        "    axes[0, 1].set_title('Uncertainty vs Prediction Error')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add correlation coefficient\n",
        "    corr_coef = np.corrcoef(uncertainties, prediction_errors)[0, 1]\n",
        "    axes[0, 1].text(0.05, 0.95, f'Correlation: {corr_coef:.3f}',\n",
        "                   transform=axes[0, 1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
        "\n",
        "    # 3. Uncertainty distribution\n",
        "    axes[1, 0].hist(uncertainties, bins=20, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 0].set_xlabel('Prediction Uncertainty (Std)')\n",
        "    axes[1, 0].set_ylabel('Density')\n",
        "    axes[1, 0].set_title('Distribution of Prediction Uncertainties')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Confidence intervals analysis\n",
        "    confidence_levels = [0.68, 0.95]  # 1 and 2\n",
        "    coverage_rates = []\n",
        "\n",
        "    for conf_level in confidence_levels:\n",
        "        z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
        "        lower_bound = predictions - z_score * uncertainties\n",
        "        upper_bound = predictions + z_score * uncertainties\n",
        "\n",
        "        coverage = np.mean((y_subset >= lower_bound) & (y_subset <= upper_bound))\n",
        "        coverage_rates.append(coverage)\n",
        "\n",
        "    axes[1, 1].bar([f'{int(conf*100)}%' for conf in confidence_levels],\n",
        "                   coverage_rates, alpha=0.7, color=['orange', 'red'])\n",
        "    axes[1, 1].axhline(y=confidence_levels[0], color='orange', linestyle='--', alpha=0.7)\n",
        "    axes[1, 1].axhline(y=confidence_levels[1], color='red', linestyle='--', alpha=0.7)\n",
        "    axes[1, 1].set_ylabel('Actual Coverage Rate')\n",
        "    axes[1, 1].set_title('Confidence Interval Coverage')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (conf, rate) in enumerate(zip(confidence_levels, coverage_rates)):\n",
        "        axes[1, 1].text(i, rate + 0.02, f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/uncertainty_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Uncertainty visualization saved to: {save_dir}/uncertainty_analysis.png\")\n",
        "\n",
        "    # plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return predictions, uncertainties\n",
        "\n",
        "def print_detailed_results_summary(results, config):\n",
        "    \"\"\"\n",
        "    Mencetak ringkasan hasil yang detail dan terstruktur\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED RESULTS SUMMARY - ENHANCED LOB TRANSFORMER\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Model Configuration Summary\n",
        "    print(f\"\\n MODEL CONFIGURATION:\")\n",
        "    print(f\"   Architecture: Enhanced LOB Transformer\")\n",
        "    print(f\"   Model Dimension: {config['d_model']}\")\n",
        "    print(f\"   Attention Heads: {config['n_heads']}\")\n",
        "    print(f\"   Encoder Layers: {config['n_layers']}\")\n",
        "    print(f\"   Feed Forward Dim: {config['d_ff']}\")\n",
        "\n",
        "    # Dropout Configuration\n",
        "    print(f\"\\n  DROPOUT CONFIGURATION:\")\n",
        "    print(f\"   Input Dropout: {config['input_dropout']:.3f}\")\n",
        "    print(f\"   General Dropout: {config['dropout']:.3f}\")\n",
        "    print(f\"   Layer Dropout: {config['layer_dropout']:.3f}\")\n",
        "    print(f\"   Output Dropout: {config['output_dropout']:.3f}\")\n",
        "\n",
        "    # Performance Metrics\n",
        "    # print(f\"\\n PERFORMANCE METRICS:\")\n",
        "    # print(f\"   {'Dataset':<12} {'MSE':<12} {'RMSE':<12} {'MAE':<12} {'R':<8}\")\n",
        "    # print(f\"   {'-'*60}\")\n",
        "    print(f\"\\n PERFORMANCE METRICS:\")\n",
        "    print(f\"   {'Dataset':<12} {'MSE':<12} {'RMSE':<12} {'MAE':<12} {'R':<8} {'Accuracy':<10}\")\n",
        "    print(f\"   {'-'*72}\")\n",
        "\n",
        "\n",
        "    for dataset_name in ['train', 'val', 'test']:\n",
        "          metrics = results[dataset_name]['metrics']\n",
        "          acc_percent = metrics.get('ThresholdAccuracy', 0) * 100\n",
        "          print(f\"   {dataset_name.capitalize():<12} \"\n",
        "                f\"{metrics['MSE']:<12.6f} \"\n",
        "                f\"{metrics['RMSE']:<12.6f} \"\n",
        "                f\"{metrics['MAE']:<12.6f} \"\n",
        "                f\"{metrics['R2']:<8.4f} \"\n",
        "                f\"{acc_percent:<.2f}%\")\n",
        "\n",
        "\n",
        "    # Overfitting Analysis\n",
        "    train_r2 = results['train']['metrics']['R2']\n",
        "    val_r2 = results['val']['metrics']['R2']\n",
        "    test_r2 = results['test']['metrics']['R2']\n",
        "\n",
        "    overfitting_gap = train_r2 - val_r2\n",
        "    generalization_gap = val_r2 - test_r2\n",
        "\n",
        "    print(f\"\\n OVERFITTING ANALYSIS:\")\n",
        "    print(f\"   Train R: {train_r2:.4f}\")\n",
        "    print(f\"   Val R:   {val_r2:.4f}\")\n",
        "    print(f\"   Test R:  {test_r2:.4f}\")\n",
        "    print(f\"   Train-Val Gap: {overfitting_gap:.4f}\")\n",
        "    print(f\"   Val-Test Gap:  {generalization_gap:.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\n INTERPRETATION:\")\n",
        "    if overfitting_gap > 0.1:\n",
        "        print(f\"     HIGH OVERFITTING DETECTED\")\n",
        "        print(f\"      Consider increasing dropout rates or reducing model complexity\")\n",
        "    elif overfitting_gap > 0.05:\n",
        "        print(f\"     MODERATE OVERFITTING\")\n",
        "        print(f\"      Dropout is helping but could be increased\")\n",
        "    else:\n",
        "        print(f\"    EXCELLENT REGULARIZATION\")\n",
        "        print(f\"      Dropout configuration is working well\")\n",
        "\n",
        "    if abs(generalization_gap) < 0.03:\n",
        "        print(f\"    Good generalization from validation to test\")\n",
        "    else:\n",
        "        print(f\"     Significant val-test gap detected\")\n",
        "\n",
        "    print(f\"\\n MODEL QUALITY ASSESSMENT:\")\n",
        "    if test_r2 > 0.8:\n",
        "        print(f\"    EXCELLENT performance (R > 0.8)\")\n",
        "    elif test_r2 > 0.6:\n",
        "        print(f\"    GOOD performance (R > 0.6)\")\n",
        "    elif test_r2 > 0.4:\n",
        "        print(f\"     FAIR performance (R > 0.4)\")\n",
        "    else:\n",
        "        print(f\"    POOR performance (R < 0.4)\")\n",
        "\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def create_all_visualizations(model, trainer, results, config, save_dir=None):\n",
        "    \"\"\"\n",
        "    Fungsi utama untuk membuat semua visualisasi\n",
        "    \"\"\"\n",
        "    print(\"\\n CREATING ALL VISUALIZATIONS...\")\n",
        "\n",
        "    # 1. Print detailed summary\n",
        "    print_detailed_results_summary(results, config)\n",
        "\n",
        "    # 2. Create comprehensive static visualization\n",
        "    create_comprehensive_visualization(model, trainer, results, config, save_dir)\n",
        "\n",
        "    # 3. Create interactive visualization\n",
        "    try:\n",
        "        create_interactive_visualization(results, save_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Interactive visualization skipped: {e}\")\n",
        "\n",
        "    print(\"\\n All standard visualizations completed!\")\n",
        "\n",
        "\n",
        "def calculate_threshold_accuracy(y_true, y_pred, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Hitung akurasi prediksi dengan toleransi error tertentu (default: 1%)\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Hindari pembagian nol\n",
        "    denominator = np.where(y_true == 0, 1e-8, y_true)\n",
        "    relative_error = np.abs(y_true - y_pred) / denominator\n",
        "    correct = relative_error <= threshold\n",
        "    return np.mean(correct)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 7. MAIN PIPELINE WITH ENHANCED MODEL\n",
        "# ================================\n",
        "\n",
        "def run_enhanced_lob_transformer(data_dir, risk_level='auto', save_dir=None):\n",
        "    \"\"\"\n",
        "    Main pipeline for enhanced LOB transformer training with comprehensive dropout\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"ENHANCED LOB TRANSFORMER - COMPREHENSIVE DROPOUT FOR OVERFITTING PREVENTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create save directory\n",
        "    if save_dir is None:\n",
        "        save_dir = '/content/drive/MyDrive/SKRIPSI/dataset_setelah_modeling/minMaxScaller/BRPT'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Load data\n",
        "    print(\"\\n1. Loading data...\")\n",
        "    data = load_preprocessed_csv_data(data_dir, debug=True)\n",
        "\n",
        "    # 2. Get enhanced configuration\n",
        "    print(\"\\n2. Enhanced Configuration...\")\n",
        "    data_info = {\n",
        "        'n_train_samples': len(data['X_train']),\n",
        "        'n_features': data['n_features'],\n",
        "        'n_timesteps': data['n_timesteps']\n",
        "    }\n",
        "\n",
        "    config = get_enhanced_config(data_info, risk_level)\n",
        "\n",
        "    print(f\"\\nFinal enhanced configuration:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    print(\"\\n3. Creating data loaders...\")\n",
        "    train_dataset = TensorDataset(data['X_train'], data['y_train'])\n",
        "    val_dataset = TensorDataset(data['X_val'], data['y_val'])\n",
        "    test_dataset = TensorDataset(data['X_test'], data['y_test'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    print(f\"  Train batches: {len(train_loader)}\")\n",
        "    print(f\"  Val batches: {len(val_loader)}\")\n",
        "    print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "    # 4. Create enhanced model\n",
        "    print(\"\\n4. Creating enhanced model with comprehensive dropout...\")\n",
        "    model = EnhancedLOBTransformer(\n",
        "        input_dim=data['n_features'],\n",
        "        d_model=config['d_model'],\n",
        "        n_heads=config['n_heads'],\n",
        "        n_layers=config['n_layers'],\n",
        "        d_ff=config['d_ff'],\n",
        "        dropout=config['dropout'],\n",
        "        input_dropout=config['input_dropout'],\n",
        "        layer_dropout=config['layer_dropout'],\n",
        "        output_dropout=config['output_dropout'],\n",
        "        max_seq_len=data['n_timesteps']\n",
        "    )\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "    # 5. Train enhanced model\n",
        "    print(\"\\n5. Training enhanced model...\")\n",
        "    trainer = EnhancedTrainer(model, train_loader, val_loader, test_loader, config)\n",
        "    results = trainer.train()\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        y_true = results[split]['targets']\n",
        "        y_pred = results[split]['predictions']\n",
        "        acc = calculate_threshold_accuracy(y_true, y_pred, threshold=0.01)\n",
        "        results[split]['metrics']['ThresholdAccuracy'] = acc\n",
        "    print(\"\\n CREATING CORE METRICS VISUALIZATION...\")\n",
        "    try:\n",
        "        create_core_metrics_visualization(results, trainer, save_dir)\n",
        "        print(\" Core metrics visualization completed!\")\n",
        "    except Exception as e:\n",
        "        print(f\" Core metrics visualization failed: {e}\")\n",
        "    # 6. Save enhanced model\n",
        "    print(\"\\n6. Saving enhanced model...\")\n",
        "    model_path = os.path.join(save_dir, 'enhanced_lob_transformer.pth')\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'data_info': data_info,\n",
        "        'model_architecture': {\n",
        "            'input_dim': data['n_features'],\n",
        "            'd_model': config['d_model'],\n",
        "            'n_heads': config['n_heads'],\n",
        "            'n_layers': config['n_layers'],\n",
        "            'd_ff': config['d_ff'],\n",
        "            'dropout': config['dropout'],\n",
        "            'input_dropout': config['input_dropout'],\n",
        "            'layer_dropout': config['layer_dropout'],\n",
        "            'output_dropout': config['output_dropout'],\n",
        "            'max_seq_len': data['n_timesteps']\n",
        "        }\n",
        "    }, model_path)\n",
        "\n",
        "    if data['scaler'] is not None:\n",
        "        scaler_path = os.path.join(save_dir, 'enhanced_model_scaler.pkl')\n",
        "        with open(scaler_path, 'wb') as f:\n",
        "            pickle.dump(data['scaler'], f)\n",
        "\n",
        "    print(f\"Enhanced model saved to: {model_path}\")\n",
        "\n",
        "    # 7. Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ENHANCED MODEL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    test_metrics = results['test']['metrics']\n",
        "    print(f\"\\nTest Set Performance:\")\n",
        "    print(f\"  MSE:  {test_metrics['MSE']:.6f}\")\n",
        "    print(f\"  RMSE: {test_metrics['RMSE']:.6f}\")\n",
        "    print(f\"  MAE:  {test_metrics['MAE']:.6f}\")\n",
        "    print(f\"  R:   {test_metrics['R2']:.4f}\")\n",
        "    print(f\"  Accuracy (1% error): {test_metrics['ThresholdAccuracy']*100:.2f}%\")\n",
        "\n",
        "\n",
        "    # Enhanced overfitting analysis\n",
        "    train_r2 = results['train']['metrics']['R2']\n",
        "    val_r2 = results['val']['metrics']['R2']\n",
        "    test_r2 = results['test']['metrics']['R2']\n",
        "\n",
        "    print(f\"\\nEnhanced Generalization Analysis:\")\n",
        "    print(f\"  Train R: {train_r2:.4f}\")\n",
        "    print(f\"  Val R:   {val_r2:.4f}\")\n",
        "    print(f\"  Test R:  {test_r2:.4f}\")\n",
        "\n",
        "    overfitting_gap = train_r2 - val_r2\n",
        "    generalization_gap = val_r2 - test_r2\n",
        "\n",
        "    print(f\"\\nDropout Effectiveness:\")\n",
        "    print(f\"  Train-Val Gap: {overfitting_gap:.4f}\")\n",
        "    print(f\"  Val-Test Gap:  {generalization_gap:.4f}\")\n",
        "\n",
        "    if overfitting_gap > 0.1:\n",
        "        print(f\"    High overfitting (consider increasing dropout)\")\n",
        "    elif overfitting_gap > 0.05:\n",
        "        print(f\"    Moderate overfitting (dropout is helping)\")\n",
        "    else:\n",
        "        print(f\"   Excellent regularization (dropout working well)\")\n",
        "\n",
        "    # ================================\n",
        "    # 8. CREATE COMPREHENSIVE VISUALIZATIONS\n",
        "    # ================================\n",
        "\n",
        "    print(\"\\n GENERATING COMPREHENSIVE VISUALIZATIONS...\")\n",
        "\n",
        "    # Create all visualizations\n",
        "    create_all_visualizations(model, trainer, results, config, save_dir)\n",
        "\n",
        "    # Optional: Create uncertainty visualization\n",
        "    try:\n",
        "        print(\"\\n Creating uncertainty analysis...\")\n",
        "        model_path = os.path.join(save_dir, 'enhanced_lob_transformer.pth')\n",
        "        scaler_path = os.path.join(save_dir, 'enhanced_model_scaler.pkl')\n",
        "        predictor = EnhancedLOBPredictor(model_path, scaler_path)\n",
        "\n",
        "        create_uncertainty_visualization(\n",
        "            predictor,\n",
        "            data['X_test'].numpy(),\n",
        "            data['y_test'].numpy(),\n",
        "            n_samples=20,\n",
        "            save_dir=save_dir\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"  Uncertainty visualization skipped: {e}\")\n",
        "\n",
        "    print(\"\\n ALL VISUALIZATIONS COMPLETED!\")\n",
        "    print(f\" Results saved to: {save_dir}\")\n",
        "\n",
        "    return model, trainer, results\n",
        "\n",
        "# ================================\n",
        "# 8. EXAMPLE USAGE\n",
        "# ================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"LOB Transformer with Comprehensive Dropout & Visualization Loaded!\")\n",
        "\n",
        "    # Example usage\n",
        "    data_dir = '/content/drive/MyDrive/SKRIPSI/dataset_setelah_preprocessing/minMaxScaller/preprocessed_data_minmax_scaller_BRPT'\n",
        "\n",
        "    # Train enhanced model with automatic visualization\n",
        "    model, trainer, results = run_enhanced_lob_transformer(\n",
        "        data_dir=data_dir,\n",
        "        risk_level='auto',  # Will automatically select appropriate dropout levels\n",
        "        save_dir='/content/drive/MyDrive/SKRIPSI/dataset_setelah_modeling/minMaxScaller/BRPT'\n",
        "    )\n",
        "\n",
        "    # Display enhanced metrics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL METRICS SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    test_metrics = results['test']['metrics']\n",
        "    print(f\"MSE:  {test_metrics['MSE']:.6f}\")\n",
        "    print(f\"RMSE: {test_metrics['RMSE']:.6f}\")\n",
        "    print(f\"MAE:  {test_metrics['MAE']:.6f}\")\n",
        "    print(f\"R:   {test_metrics['R2']:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\" All files saved to the specified directory.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8D5AbXJIqcuk",
        "BJNkMCy4v0vV",
        "OE7-HZqLaXkz",
        "hIAV9yIbPayC"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}