# -*- coding: utf-8 -*-
"""Crawler get data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qMHZLXFne3MyRoEO0q96JZxz0cFPPvTD
"""

from google.colab import drive
drive.mount('/content/drive')

"""## BRPT"""

import requests
import csv
import time
from datetime import datetime
import pytz


# URL API untuk data saham BRPT
url = "https://exodus.stockbit.com/company-price-feed/v2/orderbook/companies/BRPT"
jakarta_tz = pytz.timezone('Asia/Jakarta')

# Header untuk Authorization
headers = {
    "Authorization": "Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU3MDc0NjI3LTg4MWItNDQzZC04OTcyLTdmMmMzOTNlMzYyOSIsInR5cCI6IkpXVCJ9.eyJkYXRhIjp7InVzZSI6Im11a2hhbWFkZmFydXFhbGZhIiwiZW1hIjoiYWxmYWZhaG1pMTcyQGdtYWlsLmNvbSIsImZ1bCI6Ik11a2hhbWFkIEZhcnVxIEFsIEZhaG1pIiwic2VzIjoidjJlQklNc25ZNUdYZmZUcCIsImR2YyI6ImMzMGYxYjEwMDBhZDQxMjVhYjNjZjNmYjUzMTY2MzdlIiwidWlkIjozNTEwNTE1LCJjb3UiOiJTRyJ9LCJleHAiOjE3NTEzMzc3NTMsImlhdCI6MTc1MTI1MTM1MywiaXNzIjoiU1RPQ0tCSVQiLCJqdGkiOiIxODgwYWY5NS1jNDNlLTQ3ZGItOGZlZi01ZDQzOGRlOWE0YWIiLCJuYmYiOjE3NTEyNTEzNTMsInZlciI6InYxIn0.C-6mdXy3Fr8pJUaA3_ssXIBqYvubSUbDxeOz4c4_ANVAE398NEZSaEt8hujoHQkJvjBm7UMME_fnkfYYPz9KZjVW9ifppMCw3DlV7WeI_cnxVwIEth4Qime8wzOPguewtlnb5M9Chu3QuioiNLv0a9VdieIa6104Z_JCl4d9sKvfL_81UFSaZKu06AfKy3l4cmZXuDrNeojNMTdWdcFOhW5etBQWVdO15DmQsNXt6eeZPbMEcGRkgyDznD20UadMY4sDza5eaaIdORug0koYKgFstwJSLFulocj71jl97oQ7b1MHkWlZ31SjWRTfGKGZcyJ3KhqvlSWOQ6OeuFFa7Q",  # Ganti dengan token Bearer yang valid
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
    "X-Requested-With": "XMLHttpRequest"
}

# Fungsi untuk mengambil data dari API
def fetch_data():
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Memastikan tidak ada error pada HTTP request
        data = response.json()
        return data
    except requests.exceptions.HTTPError as err:
        print(f"Error fetching data: {err}")
        return None

# Menyaring data yang penting
def extract_relevant_data(data):
    if data and "data" in data:
        price_data = data["data"]

        # Mengambil data bid, offer, dan lainnya
        relevant_data = {
            "timestamp": datetime.now(jakarta_tz).strftime("%Y-%m-%d %H:%M:%S %Z"),
            "last_price": price_data.get("lastprice", None),
            "percentage_change": price_data.get("percentage_change", None),
            "high_price": price_data.get("high", None),
            "low_price": price_data.get("low", None),
            "bid": [],
            "offer": []
        }

        # Mengambil bid
        for bid in price_data.get("bid", []):
            relevant_data["bid"].append({
                "price": bid.get("price"),
                "volume": bid.get("volume")
            })

        # Mengambil offer
        for offer in price_data.get("offer", []):
            relevant_data["offer"].append({
                "price": offer.get("price"),
                "volume": offer.get("volume")
            })

        return relevant_data
    else:
        print("No valid data found.")
        return None

# Menyimpan data ke CSV
def save_to_csv(data, filename="lob_data_BRPT.csv"):
    # Membuat fieldnames untuk 10 level bid dan offer
    fieldnames = ["timestamp", "last_price", "percentage_change", "high_price", "low_price"]

    # Menambahkan fieldnames untuk bid dan offer level 1-10
    for i in range(1, 11):
        fieldnames.extend([
            f"bid_price_{i}",
            f"bid_volume_{i}",
            f"offer_price_{i}",
            f"offer_volume_{i}"
        ])

    with open(filename, mode="a", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # Menulis header jika file baru
        file_exists = file.tell() > 0
        if not file_exists:
            writer.writeheader()

        # Menulis data yang sudah disaring
        if data:
            row = {
                "timestamp": data["timestamp"],
                "last_price": data["last_price"],
                "percentage_change": data["percentage_change"],
                "high_price": data["high_price"],
                "low_price": data["low_price"]
            }

            # Mengisi data untuk 10 level bid dan offer
            for i in range(10):
                # Data bid
                if i < len(data["bid"]):
                    row[f"bid_price_{i+1}"] = data["bid"][i]["price"]
                    row[f"bid_volume_{i+1}"] = data["bid"][i]["volume"]
                else:
                    row[f"bid_price_{i+1}"] = None
                    row[f"bid_volume_{i+1}"] = None

                # Data offer
                if i < len(data["offer"]):
                    row[f"offer_price_{i+1}"] = data["offer"][i]["price"]
                    row[f"offer_volume_{i+1}"] = data["offer"][i]["volume"]
                else:
                    row[f"offer_price_{i+1}"] = None
                    row[f"offer_volume_{i+1}"] = None

            writer.writerow(row)

# Crawler berjalan untuk 60 menit
def run_crawler(duration=60):
    print(f"Starting crawler for {duration} minutes...")

    for minute in range(duration):
        print(f"[{datetime.now()}] Fetching data minute {minute + 1}/{duration}")

        data = fetch_data()
        if data:
            relevant_data = extract_relevant_data(data)
            save_to_csv(relevant_data)

        # Menunggu 1 menit sebelum mengambil data lagi
        time.sleep(60)

    print(f"[{datetime.now()}] Crawler finished!")

# Menjalankan crawler
run_crawler()

"""## CUAN"""

import requests
import csv
import time
from datetime import datetime
import pytz


# URL API untuk data saham BRPT
url = "https://exodus.stockbit.com/company-price-feed/v2/orderbook/companies/CUAN"
jakarta_tz = pytz.timezone('Asia/Jakarta')
# Header untuk Authorization
headers = {
    "Authorization": "Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU3MDc0NjI3LTg4MWItNDQzZC04OTcyLTdmMmMzOTNlMzYyOSIsInR5cCI6IkpXVCJ9.eyJkYXRhIjp7InVzZSI6Im11a2hhbWFkZmFydXFhbGZhIiwiZW1hIjoiYWxmYWZhaG1pMTcyQGdtYWlsLmNvbSIsImZ1bCI6Ik11a2hhbWFkIEZhcnVxIEFsIEZhaG1pIiwic2VzIjoidjJlQklNc25ZNUdYZmZUcCIsImR2YyI6ImMzMGYxYjEwMDBhZDQxMjVhYjNjZjNmYjUzMTY2MzdlIiwidWlkIjozNTEwNTE1LCJjb3UiOiJJRCJ9LCJleHAiOjE3NTE0MjQ0NjYsImlhdCI6MTc1MTMzODA2NiwiaXNzIjoiU1RPQ0tCSVQiLCJqdGkiOiI1Yjk3MTc0MS0wMjUxLTRlNjMtODU0Ni00ZjQyMWM5MGU5YjgiLCJuYmYiOjE3NTEzMzgwNjYsInZlciI6InYxIn0.HREgt19WcXJflhYhxiA_QvHbzdwZ2oLi5k18BdCvkMreDaz8yEwrBmXUxCRf67FYs5viaKHP5nME37x1L8jliz7njv5i_9TIr2NpaohLrT_v-Kz2QelVyacKuOWeLUU4OwpRbjmp1axb54pPq9-2nXT3gt2L332qEA01xxchRq8dAMAmCeU5WTzH9_NP2-pbmK7dzIQc9A_i4ratuTSZIQBtr0Qpf3nb3oXhSVNnEpfJybQ6XEpZutrMd3yLwecJPDj1EI2tU1l6xvXjWu7kwDjjACsWeSWRxdD8icImIVRIoMxaNPiwS5Cnc6s6g3J08iNcaoG3zoXkEHAdj5oqww",
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
    "X-Requested-With": "XMLHttpRequest"
}

# Fungsi untuk mengambil data dari API
def fetch_data():
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Memastikan tidak ada error pada HTTP request
        data = response.json()
        return data
    except requests.exceptions.HTTPError as err:
        print(f"Error fetching data: {err}")
        return None

# Menyaring data yang penting
def extract_relevant_data(data):
    if data and "data" in data:
        price_data = data["data"]

        # Mengambil data bid, offer, dan lainnya
        relevant_data = {
            "timestamp": datetime.now(jakarta_tz).strftime("%Y-%m-%d %H:%M:%S %Z"),
            "last_price": price_data.get("lastprice", None),
            "percentage_change": price_data.get("percentage_change", None),
            "high_price": price_data.get("high", None),
            "low_price": price_data.get("low", None),
            "bid": [],
            "offer": []
        }

        # Mengambil bid
        for bid in price_data.get("bid", []):
            relevant_data["bid"].append({
                "price": bid.get("price"),
                "volume": bid.get("volume")
            })

        # Mengambil offer
        for offer in price_data.get("offer", []):
            relevant_data["offer"].append({
                "price": offer.get("price"),
                "volume": offer.get("volume")
            })

        return relevant_data
    else:
        print("No valid data found.")
        return None

# Menyimpan data ke CSV
def save_to_csv(data, filename="lob_data_CUAN.csv"):
    # Membuat fieldnames untuk 10 level bid dan offer
    fieldnames = ["timestamp", "last_price", "percentage_change", "high_price", "low_price"]

    # Menambahkan fieldnames untuk bid dan offer level 1-10
    for i in range(1, 11):
        fieldnames.extend([
            f"bid_price_{i}",
            f"bid_volume_{i}",
            f"offer_price_{i}",
            f"offer_volume_{i}"
        ])

    with open(filename, mode="a", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # Menulis header jika file baru
        file_exists = file.tell() > 0
        if not file_exists:
            writer.writeheader()

        # Menulis data yang sudah disaring
        if data:
            row = {
                "timestamp": data["timestamp"],
                "last_price": data["last_price"],
                "percentage_change": data["percentage_change"],
                "high_price": data["high_price"],
                "low_price": data["low_price"]
            }

            # Mengisi data untuk 10 level bid dan offer
            for i in range(10):
                # Data bid
                if i < len(data["bid"]):
                    row[f"bid_price_{i+1}"] = data["bid"][i]["price"]
                    row[f"bid_volume_{i+1}"] = data["bid"][i]["volume"]
                else:
                    row[f"bid_price_{i+1}"] = None
                    row[f"bid_volume_{i+1}"] = None

                # Data offer
                if i < len(data["offer"]):
                    row[f"offer_price_{i+1}"] = data["offer"][i]["price"]
                    row[f"offer_volume_{i+1}"] = data["offer"][i]["volume"]
                else:
                    row[f"offer_price_{i+1}"] = None
                    row[f"offer_volume_{i+1}"] = None

            writer.writerow(row)

# Crawler berjalan untuk 60 menit
def run_crawler(duration=60):
    print(f"Starting crawler for {duration} minutes...")

    for minute in range(duration):
        print(f"[{datetime.now()}] Fetching data minute {minute + 1}/{duration}")

        data = fetch_data()
        if data:
            relevant_data = extract_relevant_data(data)
            save_to_csv(relevant_data)\

        # Menunggu 1 menit sebelum mengambil data lagi
        time.sleep(60)

    print(f"[{datetime.now()}] Crawler finished!")

# Menjalankan crawler
run_crawler()

"""## TPIA"""

import requests
import csv
import time
from datetime import datetime
import pytz

# URL API untuk data saham TPIA
url = "https://exodus.stockbit.com/company-price-feed/v2/orderbook/companies/TPIA"
jakarta_tz = pytz.timezone('Asia/Jakarta')

# Header untuk Authorization
headers = {
    "Authorization": "Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU3MDc0NjI3LTg4MWItNDQzZC04OTcyLTdmMmMzOTNlMzYyOSIsInR5cCI6IkpXVCJ9.eyJkYXRhIjp7InVzZSI6Im11a2hhbWFkZmFydXFhbGZhIiwiZW1hIjoiYWxmYWZhaG1pMTcyQGdtYWlsLmNvbSIsImZ1bCI6Ik11a2hhbWFkIEZhcnVxIEFsIEZhaG1pIiwic2VzIjoiNXR0NHJxTzBkaUUzb1NGYyIsImR2YyI6ImMzMGYxYjEwMDBhZDQxMjVhYjNjZjNmYjUzMTY2MzdlIiwidWlkIjozNTEwNTE1LCJjb3UiOiJTRyJ9LCJleHAiOjE3NTE1MTM2NTIsImlhdCI6MTc1MTQyNzI1MiwiaXNzIjoiU1RPQ0tCSVQiLCJqdGkiOiJhMDNiOWYxNy0yMmQwLTQyZGMtODY3Zi0xMTI2NzA2NTJlMTQiLCJuYmYiOjE3NTE0MjcyNTIsInZlciI6InYxIn0.TfA-fJYJ_EaYRfwXkhoPDOQjZmmU2zXIiSni4Ly7DqAytbfErINko1Q3L8VAzBBEZJeg9NNC7fLVT3nFyJPdxPDNSnZ4esqzahCt_InhY7awHllNECm_FTH8qsuRnEkeFKykKSFmz1Hb_EXojEFuyElLuHIyXkAqeG_5q-TpX2fQANubfztHdC6LmHCeSH0EniqKi33iX734gnPBvd4PKZcR9J2VmmJBpKWcOUmqXbRhGkB2cBAi2SuLeRZTeeL2fQCzR8bEHGWE31spExepXa6pmyFiUBQ9INn2my-Ac5qFrzC19soddmcPKKfrieYnvIwoC1dTV1LGPNhgtqW1lQ",
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
    "X-Requested-With": "XMLHttpRequest"
}

# Fungsi untuk mengambil data dari API
def fetch_data():
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Memastikan tidak ada error pada HTTP request
        data = response.json()
        return data
    except requests.exceptions.HTTPError as err:
        print(f"Error fetching data: {err}")
        return None

# Menyaring data yang penting
def extract_relevant_data(data):
    if data and "data" in data:
        price_data = data["data"]

        # Mengambil data bid, offer, dan lainnya
        relevant_data = {
            "timestamp": datetime.now(jakarta_tz).strftime("%Y-%m-%d %H:%M:%S %Z"),
            "last_price": price_data.get("lastprice", None),
            "percentage_change": price_data.get("percentage_change", None),
            "high_price": price_data.get("high", None),
            "low_price": price_data.get("low", None),
            "bid": [],
            "offer": []
        }

        # Mengambil bid
        for bid in price_data.get("bid", []):
            relevant_data["bid"].append({
                "price": bid.get("price"),
                "volume": bid.get("volume")
            })

        # Mengambil offer
        for offer in price_data.get("offer", []):
            relevant_data["offer"].append({
                "price": offer.get("price"),
                "volume": offer.get("volume")
            })

        return relevant_data
    else:
        print("No valid data found.")
        return None

# Menyimpan data ke CSV
def save_to_csv(data, filename="lob_data_TPIA.csv"):
    # Membuat fieldnames untuk 10 level bid dan offer
    fieldnames = ["timestamp", "last_price", "percentage_change", "high_price", "low_price"]

    # Menambahkan fieldnames untuk bid dan offer level 1-10
    for i in range(1, 11):
        fieldnames.extend([
            f"bid_price_{i}",
            f"bid_volume_{i}",
            f"offer_price_{i}",
            f"offer_volume_{i}"
        ])

    with open(filename, mode="a", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # Menulis header jika file baru
        file_exists = file.tell() > 0
        if not file_exists:
            writer.writeheader()

        # Menulis data yang sudah disaring
        if data:
            row = {
                "timestamp": data["timestamp"],
                "last_price": data["last_price"],
                "percentage_change": data["percentage_change"],
                "high_price": data["high_price"],
                "low_price": data["low_price"]
            }

            # Mengisi data untuk 10 level bid dan offer
            for i in range(10):
                # Data bid
                if i < len(data["bid"]):
                    row[f"bid_price_{i+1}"] = data["bid"][i]["price"]
                    row[f"bid_volume_{i+1}"] = data["bid"][i]["volume"]
                else:
                    row[f"bid_price_{i+1}"] = None
                    row[f"bid_volume_{i+1}"] = None

                # Data offer
                if i < len(data["offer"]):
                    row[f"offer_price_{i+1}"] = data["offer"][i]["price"]
                    row[f"offer_volume_{i+1}"] = data["offer"][i]["volume"]
                else:
                    row[f"offer_price_{i+1}"] = None
                    row[f"offer_volume_{i+1}"] = None

            writer.writerow(row)

# Crawler berjalan untuk 60 menit
def run_crawler(duration=60):
    print(f"Starting crawler for {duration} minutes...")

    for minute in range(duration):
        print(f"[{datetime.now()}] Fetching data minute {minute + 1}/{duration}")

        data = fetch_data()
        if data:
            relevant_data = extract_relevant_data(data)
            save_to_csv(relevant_data)

        # Menunggu 1 menit sebelum mengambil data lagi
        time.sleep(60)

    print(f"[{datetime.now()}] Crawler finished!")

# Menjalankan crawler
run_crawler()

"""## BREN"""

import requests
import csv
import time
from datetime import datetime
import pytz

# URL API untuk data saham BRPT
url = "https://exodus.stockbit.com/company-price-feed/v2/orderbook/companies/BREN"
jakarta_tz = pytz.timezone('Asia/Jakarta')

# Header untuk Authorization
headers = {
    "Authorization": "Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU3MDc0NjI3LTg4MWItNDQzZC04OTcyLTdmMmMzOTNlMzYyOSIsInR5cCI6IkpXVCJ9.eyJkYXRhIjp7InVzZSI6Im11a2hhbWFkZmFydXFhbGZhIiwiZW1hIjoiYWxmYWZhaG1pMTcyQGdtYWlsLmNvbSIsImZ1bCI6Ik11a2hhbWFkIEZhcnVxIEFsIEZhaG1pIiwic2VzIjoiVnp4V0dlcU9INFgySHVZNCIsImR2YyI6ImMzMGYxYjEwMDBhZDQxMjVhYjNjZjNmYjUzMTY2MzdlIiwidWlkIjozNTEwNTE1LCJjb3UiOiJJRCJ9LCJleHAiOjE3NTA5OTY5MDEsImlhdCI6MTc1MDkxMDUwMSwiaXNzIjoiU1RPQ0tCSVQiLCJqdGkiOiJkYmE1YzU0ZC0xMTUyLTQ0ZGYtOWExOC05Y2U5NmIxNDlmOWYiLCJuYmYiOjE3NTA5MTA1MDEsInZlciI6InYxIn0.mejBGv_sjOg55Rq6HLAnKVvc_7emepSt73e8lCsqGoYWkzZKxrCRgL9kaCdIOnpmxaQWf8TO2K73ApPepTDxGC48GM-EWBL7iUrZzYuWKCy3hkMqHABiy3OMCJdaRtM0GUtqXnZ5j0PjdUHXta6TN0Rl7C7nnPLRt2duDCBWTlujq_v22zH-ZZL4xCX1EaB1ALGewEYQi52Jq6od8vHBfLqrZSAOHm38sFJH-OJXyzPEfVP4WM4w45nH7ya1xIy6XfSethZ-C_YkeuVNCN5Vvh9jEsTwFoS1OBvuMvFXY6etXnw_uYeML7AskSo88gl6L7E8Or9WgXGrHWfp6ZLFlg",
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
    "X-Requested-With": "XMLHttpRequest"
}

# Fungsi untuk mengambil data dari API
def fetch_data():
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Memastikan tidak ada error pada HTTP request
        data = response.json()
        return data
    except requests.exceptions.HTTPError as err:
        print(f"Error fetching data: {err}")
        return None

# Menyaring data yang penting
def extract_relevant_data(data):
    if data and "data" in data:
        price_data = data["data"]

        # Mengambil data bid, offer, dan lainnya
        relevant_data = {
            "timestamp": datetime.now(jakarta_tz).strftime("%Y-%m-%d %H:%M:%S %Z"),
            "last_price": price_data.get("lastprice", None),
            "percentage_change": price_data.get("percentage_change", None),
            "high_price": price_data.get("high", None),
            "low_price": price_data.get("low", None),
            "bid": [],
            "offer": []
        }

        # Mengambil bid
        for bid in price_data.get("bid", []):
            relevant_data["bid"].append({
                "price": bid.get("price"),
                "volume": bid.get("volume")
            })

        # Mengambil offer
        for offer in price_data.get("offer", []):
            relevant_data["offer"].append({
                "price": offer.get("price"),
                "volume": offer.get("volume")
            })

        return relevant_data
    else:
        print("No valid data found.")
        return None

# Menyimpan data ke CSV
def save_to_csv(data, filename="lob_data_BREN.csv"):
    # Membuat fieldnames untuk 10 level bid dan offer
    fieldnames = ["timestamp", "last_price", "percentage_change", "high_price", "low_price"]

    # Menambahkan fieldnames untuk bid dan offer level 1-10
    for i in range(1, 11):
        fieldnames.extend([
            f"bid_price_{i}",
            f"bid_volume_{i}",
            f"offer_price_{i}",
            f"offer_volume_{i}"
        ])

    with open(filename, mode="a", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # Menulis header jika file baru
        file_exists = file.tell() > 0
        if not file_exists:
            writer.writeheader()

        # Menulis data yang sudah disaring
        if data:
            row = {
                "timestamp": data["timestamp"],
                "last_price": data["last_price"],
                "percentage_change": data["percentage_change"],
                "high_price": data["high_price"],
                "low_price": data["low_price"]
            }

            # Mengisi data untuk 10 level bid dan offer
            for i in range(10):
                # Data bid
                if i < len(data["bid"]):
                    row[f"bid_price_{i+1}"] = data["bid"][i]["price"]
                    row[f"bid_volume_{i+1}"] = data["bid"][i]["volume"]
                else:
                    row[f"bid_price_{i+1}"] = None
                    row[f"bid_volume_{i+1}"] = None

                # Data offer
                if i < len(data["offer"]):
                    row[f"offer_price_{i+1}"] = data["offer"][i]["price"]
                    row[f"offer_volume_{i+1}"] = data["offer"][i]["volume"]
                else:
                    row[f"offer_price_{i+1}"] = None
                    row[f"offer_volume_{i+1}"] = None

            writer.writerow(row)

# Crawler berjalan untuk 60 menit
def run_crawler(duration=60):
    print(f"Starting crawler for {duration} minutes...")

    for minute in range(duration):
        print(f"[{datetime.now()}] Fetching data minute {minute + 1}/{duration}")

        data = fetch_data()
        if data:
            relevant_data = extract_relevant_data(data)
            save_to_csv(relevant_data)

        # Menunggu 1 menit sebelum mengambil data lagi
        time.sleep(60)

    print(f"[{datetime.now()}] Crawler finished!")

# Menjalankan crawler
run_crawler()

"""## PTRO"""

import requests
import csv
import time
from datetime import datetime
import pytz

# URL API untuk data saham BRPT
url = "https://exodus.stockbit.com/company-price-feed/v2/orderbook/companies/PTRO"
jakarta_tz = pytz.timezone('Asia/Jakarta')
# Header untuk Authorization
headers = {
    "Authorization": "Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU3MDc0NjI3LTg4MWItNDQzZC04OTcyLTdmMmMzOTNlMzYyOSIsInR5cCI6IkpXVCJ9.eyJkYXRhIjp7InVzZSI6Im11a2hhbWFkZmFydXFhbGZhIiwiZW1hIjoiYWxmYWZhaG1pMTcyQGdtYWlsLmNvbSIsImZ1bCI6Ik11a2hhbWFkIEZhcnVxIEFsIEZhaG1pIiwic2VzIjoiekVFTTJpU2lmdERvdTZ0YSIsImR2YyI6ImMzMGYxYjEwMDBhZDQxMjVhYjNjZjNmYjUzMTY2MzdlIiwidWlkIjozNTEwNTE1LCJjb3UiOiJTRyJ9LCJleHAiOjE3NTE2OTQ4NjQsImlhdCI6MTc1MTYwODQ2NCwiaXNzIjoiU1RPQ0tCSVQiLCJqdGkiOiJmYjlmZGNhMS1kYjlhLTQ3MDItYmRmYy0wNTk1OWU1NTkzY2YiLCJuYmYiOjE3NTE2MDg0NjQsInZlciI6InYxIn0.vVbOFq--8oQx2rSQaz51SJVlLMPtiXbJTM2XHygCXyD63XpVMnMHioy1p_7bQOn5_D3sliOsgTa-KIDho5bfVMUX_U4qzKeXDaVaNfOsgrh-8q3oqtqlxAl_ZAE7gFMFXt7yCI0n6EOBopAgF7Nl3Q4IrmIUIfNaZim1_w3k5B0Mt8IDQlGttICzeLWK9vveMoKvIRdDgDLOEqnqeLEL4z8IGc-P_kfbqMuFRGGeO1bDdwla6PwfB3fPRi5yc05o359Q5LBNSaXcdxGQD0whlsA6R9SKAsZiGE74J11Z9rCXOs6rL11CKewzgS9myUxoZoInmJsLNdqXeXEtFhKxbg",
    "User-Agent": "Mozilla/5.0",
    "Accept": "application/json",
    "X-Requested-With": "XMLHttpRequest"
}

# Fungsi untuk mengambil data dari API
def fetch_data():
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Memastikan tidak ada error pada HTTP request
        data = response.json()
        return data
    except requests.exceptions.HTTPError as err:
        print(f"Error fetching data: {err}")
        return None

# Menyaring data yang penting
def extract_relevant_data(data):
    if data and "data" in data:
        price_data = data["data"]

        # Mengambil data bid, offer, dan lainnya
        relevant_data = {
            "timestamp": datetime.now(jakarta_tz).strftime("%Y-%m-%d %H:%M:%S %Z"),
            "last_price": price_data.get("lastprice", None),
            "percentage_change": price_data.get("percentage_change", None),
            "high_price": price_data.get("high", None),
            "low_price": price_data.get("low", None),
            "bid": [],
            "offer": []
        }

        # Mengambil bid
        for bid in price_data.get("bid", []):
            relevant_data["bid"].append({
                "price": bid.get("price"),
                "volume": bid.get("volume")
            })

        # Mengambil offer
        for offer in price_data.get("offer", []):
            relevant_data["offer"].append({
                "price": offer.get("price"),
                "volume": offer.get("volume")
            })

        return relevant_data
    else:
        print("No valid data found.")
        return None

# Menyimpan data ke CSV
def save_to_csv(data, filename="lob_data_PTRO.csv"):
    # Membuat fieldnames untuk 10 level bid dan offer
    fieldnames = ["timestamp", "last_price", "percentage_change", "high_price", "low_price"]

    # Menambahkan fieldnames untuk bid dan offer level 1-10
    for i in range(1, 11):
        fieldnames.extend([
            f"bid_price_{i}",
            f"bid_volume_{i}",
            f"offer_price_{i}",
            f"offer_volume_{i}"
        ])

    with open(filename, mode="a", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)

        # Menulis header jika file baru
        file_exists = file.tell() > 0
        if not file_exists:
            writer.writeheader()

        # Menulis data yang sudah disaring
        if data:
            row = {
                "timestamp": data["timestamp"],
                "last_price": data["last_price"],
                "percentage_change": data["percentage_change"],
                "high_price": data["high_price"],
                "low_price": data["low_price"]
            }

            # Mengisi data untuk 10 level bid dan offer
            for i in range(10):
                # Data bid
                if i < len(data["bid"]):
                    row[f"bid_price_{i+1}"] = data["bid"][i]["price"]
                    row[f"bid_volume_{i+1}"] = data["bid"][i]["volume"]
                else:
                    row[f"bid_price_{i+1}"] = None
                    row[f"bid_volume_{i+1}"] = None

                # Data offer
                if i < len(data["offer"]):
                    row[f"offer_price_{i+1}"] = data["offer"][i]["price"]
                    row[f"offer_volume_{i+1}"] = data["offer"][i]["volume"]
                else:
                    row[f"offer_price_{i+1}"] = None
                    row[f"offer_volume_{i+1}"] = None

            writer.writerow(row)

# Crawler berjalan untuk 60 menit
def run_crawler(duration=60):
    print(f"Starting crawler for {duration} minutes...")

    for minute in range(duration):
        print(f"[{datetime.now()}] Fetching data minute {minute + 1}/{duration}")

        data = fetch_data()
        if data:
            relevant_data = extract_relevant_data(data)
            save_to_csv(relevant_data)

        # Menunggu 1 menit sebelum mengambil data lagi
        time.sleep(60)

    print(f"[{datetime.now()}] Crawler finished!")

# Menjalankan crawler
run_crawler()